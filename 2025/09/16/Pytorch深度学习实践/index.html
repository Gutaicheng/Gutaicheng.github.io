

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tubiao2.png">
  <link rel="icon" href="/img/tubiao2.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GuTaicheng">
  <meta name="keywords" content="博客, 学习, Java">
  
    <meta name="description" content="本博客是根据Bilibil《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili所做的笔记 名词解释 数据集(Data Set)  训练集(Traing Set)  已知输入x和输出y，用于训练模型  训练集可在分一部分出来作为开发集，用于评估模型的泛化能力，防止过拟合    测试集(Test Set)  只知道输入x     过拟合(Overfitting)  是模型在训练集">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch深度学习实践">
<meta property="og:url" content="https://blog.gutaicheng.top/2025/09/16/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="GuTaicheng&#39;s Blog">
<meta property="og:description" content="本博客是根据Bilibil《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili所做的笔记 名词解释 数据集(Data Set)  训练集(Traing Set)  已知输入x和输出y，用于训练模型  训练集可在分一部分出来作为开发集，用于评估模型的泛化能力，防止过拟合    测试集(Test Set)  只知道输入x     过拟合(Overfitting)  是模型在训练集">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250910122424933.png">
<meta property="article:published_time" content="2025-09-15T16:50:00.000Z">
<meta property="article:modified_time" content="2025-10-11T13:54:50.751Z">
<meta property="article:author" content="GuTaicheng">
<meta property="article:tag" content="Pytorch - 深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250910122424933.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Pytorch深度学习实践 - GuTaicheng&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.gutaicheng.top","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":21404155,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    <!-- 51.la Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('//js.users.51.la/21404155.js');
      }
    </script>
  

  

  



  
  <meta name="referrer" content="never">
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GuTaicheng</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Pytorch深度学习实践"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-09-16 00:50" pubdate>
          2025年9月16日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          20k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          163 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Pytorch深度学习实践</h1>
            
            
              <div class="markdown-body">
                
                <meta name="referrer" content="no-referrer" />



<p>本博客是根据Bilibil<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Y7411d7Ys/">《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili</a>所做的笔记</p>
<h1 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1><ul>
<li><p>数据集(Data Set)</p>
<ul>
<li><p>训练集(Traing Set)</p>
<ul>
<li><p>已知<strong>输入x</strong>和<strong>输出y</strong>，用于训练模型</p>
</li>
<li><p>训练集可在分一部分出来作为<strong>开发集</strong>，用于评估模型的泛化能力，防止过拟合</p>
</li>
</ul>
</li>
<li><p>测试集(Test Set)</p>
<ul>
<li>只知道<strong>输入x</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>过拟合(Overfitting)</p>
<ul>
<li>是模型在训练集上表现很好，但在新数据上表现很差</li>
<li>但它并没有真正学到数据背后的普遍规律，而是把训练数据中的<strong>所有特征，包括噪声</strong>，都当作了学习目标。当它遇到<strong>新数据</strong>时，它的表现会急剧下降，预测结果非常不准确</li>
</ul>
</li>
<li><p>泛化(Generalization)</p>
<ul>
<li>泛化是机器学习模型的真正目标，它是指模型在<strong>未见过的新数据</strong>上的表现能力</li>
</ul>
</li>
</ul>
<h1 id="线性模型-Linear-Model"><a href="#线性模型-Linear-Model" class="headerlink" title="线性模型(Linear Model)"></a>线性模型(Linear Model)</h1><p>拿到数据集，先试着用线性模型<strong>试一下</strong>，即找到一个<strong>权重w和截距b</strong>，使得训练集满足：</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915205727712.png" srcset="/img/loading.gif" lazyload alt="image-20250915205727712"></p>
<ul>
<li><p><strong>ŷ</strong>读作y_hat，表示预测的值，并不是准确值</p>
</li>
<li><p>为了方便学习，这里选择简化模型，将截距b去掉</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915210137135.png" srcset="/img/loading.gif" lazyload alt="image-20250915210137135"></p>
</li>
</ul>
<p>所以线性模型的关键是找到一个合适的<strong>权重w</strong>，而什么叫做”合适呢“？即误差小。</p>
<ul>
<li><p>初始数据集可能并不是严格的在直线上的点集，会是离散的，而我们预设的线性模型是一条严格的直线</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915211004528.png" srcset="/img/loading.gif" lazyload alt="image-20250915211004528"></p>
</li>
<li><p>而预测的**ŷ(a)<strong>会和对应的同一个横坐标a的真实值</strong>y(a)**有差值</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915211334561.png" srcset="/img/loading.gif" lazyload alt="image-20250915211334561"></p>
</li>
<li><p>但是由于差值有正有负，所以对差值进行<strong>求平方</strong>（当然也有其他方法，取绝对值等，<strong>但是平方对于误差大的惩罚越大</strong>）即可消去负值影响，而这些平方值即为<strong>损失(loss)</strong></p>
<ul>
<li>loss只是针对单独一个样本的</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915212029351.png" srcset="/img/loading.gif" lazyload alt="image-20250915212029351"></p>
</li>
<li><p>得到单独样本<strong>损失</strong>后，对所有样本的损失进行<strong>求和</strong>，再取<strong>平均值</strong>，即得到训练集的<strong>误差(Error)</strong></p>
<ul>
<li>采用平方来计算这种方法得到的误差叫做<strong>平均平方误差</strong>或者<strong>均方误差</strong>（Mean Squared Error, <strong>MSE</strong>）</li>
<li>误差(Error)是针对训练集training set的</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915213243814.png" srcset="/img/loading.gif" lazyload alt="image-20250915213243814"></p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>注意：<br>Loss Function (损失函数)是衡量模型在<strong>单个数据点</strong>上的表现有多差，比如上述差值的平方</p>
<p>Cost Function (成本函数 或 目标函数)是衡量模型在<strong>整个数据集</strong>上的总体表现有多差，比如MSE</p>
</blockquote>
<blockquote>
<p>虽然技术上有区别，但在实际的工程对话中，人们经常用 <strong>Loss</strong> 来泛指<strong>整个优化过程中的误差指标</strong>（也就是成本函数 Cost），例如：“我们看一下训练 Loss 是多少。”</p>
</blockquote>
</li>
<li><p>最终找到一个误差最小的<strong>权重w</strong>，就是线性模型的关键。</p>
<p>比如下图的数据集与权重选择</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915213736813.png" srcset="/img/loading.gif" lazyload alt="image-20250915213736813"></p>
</li>
</ul>
<h2 id="穷举法找权重w"><a href="#穷举法找权重w" class="headerlink" title="穷举法找权重w"></a>穷举法找权重w</h2><ul>
<li><p>先随机找一个<strong>大的步长</strong>范围，尝试找到一个<strong>可能（可能有增或减的趋势）</strong>存在最小权重的范围</p>
</li>
<li><p>再在这个范围内，<strong>缩短步长</strong>，以此找到产生最小误差的权重</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915214623127.png" srcset="/img/loading.gif" lazyload alt="image-20250915214623127"></p>
</li>
<li><p>代码演示，绘制图如上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>] <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>] <span class="hljs-comment"># 输出</span><br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># MES损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MSE</span>(<span class="hljs-params">xd, yd, w</span>):<br>    loss_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        loss_sum += loss(x_val, y_val, w)<br>    <span class="hljs-keyword">return</span> loss_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 权重值列表</span><br>w_list = []<br><span class="hljs-comment"># MSE误差列表</span><br>mse_list = []<br><br><span class="hljs-comment"># w权重取值范围在 [0.0, 4.0], 步长间隔为 0.1</span><br><span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0.0</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">0.1</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w=&#x27;</span>, w)<br>    mes = MSE(x_data, y_data, w)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;MSE=&#x27;</span>, mes)<br>    w_list.append(w)<br>    mse_list.append(mes)<br><br>plt.plot(w_list, mse_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;w&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
</li>
<li><p>非简化版本，基本模型：<strong>ŷ &#x3D; x*w + b</strong>;   w，b，mse，三维绘制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> cm<br><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D  <span class="hljs-comment"># 导入3D绘图模块</span><br><br><span class="hljs-comment"># --- 添加这三行，设置中文显示 ---</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]  <span class="hljs-comment"># 指定默认字体为黑体</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span>     <span class="hljs-comment"># 解决保存图像时负号 &#x27;-&#x27; 显示为方块的问题</span><br><span class="hljs-comment"># --------------------------------</span><br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>] <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>] <span class="hljs-comment"># 输出</span><br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-keyword">return</span> x * w + b<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w, b</span>):<br>    y_pred = forward(x, w, b)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># MES损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MSE</span>(<span class="hljs-params">xd, yd, w, b</span>):<br>    loss_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        loss_val = loss(x_val, y_val, w, b)<br>        loss_sum += loss_val<br>    <span class="hljs-keyword">return</span> loss_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 定义 w 和 b 的取值范围</span><br><span class="hljs-comment"># arange是 Numpy 库中的一个函数，它类似于 Python 内置的 range() 函数，但主要用于创建包含浮点数的数组。</span><br><span class="hljs-comment"># 第一行代码会创建一个从 0.0 开始，到 4.1 结束（不包含 4.1），步长为 0.1 的一维数组。</span><br><span class="hljs-comment"># 第二行代码会创建一个从 -2.0 到 2.1 结束（不包含 2.1），步长为 0.1 的数组。</span><br>w_range = np.arange(<span class="hljs-number">0.0</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">0.1</span>)<br>b_range = np.arange(-<span class="hljs-number">2.0</span>, <span class="hljs-number">2.1</span>, <span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 创建一个用于存储 MSE 值的二维网格</span><br><span class="hljs-comment"># 分别创建两个矩阵 w_values 和 b_values， 用于绘制 3D 图时的 x, y 坐标</span><br><span class="hljs-comment"># w_values：这个矩阵的每一 行 都是 w_range 数组的重复</span><br><span class="hljs-comment"># b_values：这个矩阵的每一 列 都是 b_range 数组的重复。</span><br><span class="hljs-comment"># 故此将两个矩阵重叠，相同位置的两个元素(w,b)就代表了所有可能的点</span><br>w_values, b_values = np.meshgrid(w_range, b_range)<br><br><span class="hljs-comment"># 创建全零矩阵保存所有的损失值</span><br><span class="hljs-comment"># np.zeros_like()：这个函数会创建一个与 w_values 形状完全相同（即行列数一样）的全零矩阵。</span><br>mse_values = np.zeros_like(w_values)<br><br><span class="hljs-comment"># 遍历 w 和 b 的所有组合，计算并存储 MSE 值</span><br><span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(w_range):<br>    <span class="hljs-keyword">for</span> j, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(b_range):<br>        mse_values[j, i] = MSE(x_data, y_data, w, b)<br><br><span class="hljs-comment"># 绘制 3D 图像</span><br><span class="hljs-comment"># figure函数 创建一个新的图形窗口</span><br><span class="hljs-comment"># figsize 参数设置了窗口的尺寸（长和宽），当然figsize不填也行</span><br>fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br><br><span class="hljs-comment"># add_subplot()函数 在图形窗口中添加一个子图（subplot）</span><br><span class="hljs-comment"># 111 表示将图形窗口划分为 1x1 的网格，并在第 1 个子图上绘图</span><br><span class="hljs-comment"># 下列方式是简便的工厂模式。告诉 add_subplot 函数你想要一个 3D 坐标系，它就会在后台为你创建一个 Axes3D 对象。</span><br><span class="hljs-comment"># projection=&#x27;3d&#x27; 告诉 Matplotlib 要创建一个 3D 坐标系，而不是默认的 2D 坐标系</span><br><span class="hljs-comment"># 这种方式代码更简洁，但可能会让 IDE 产生误判，导致最开始的import显示未引用</span><br>ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br><br><span class="hljs-comment"># 绘制曲面图</span><br><span class="hljs-comment"># plot_surface() 绘制曲面图函数</span><br><span class="hljs-comment"># w_values, b_values, mse_values：这三个参数是 3D 表面图的三个坐标轴：x 轴、y 轴和 z 轴</span><br><span class="hljs-comment"># cmap=cm.viridis：cmap 是 &quot;colormap&quot;（颜色映射）的缩写，它定义了曲面图的颜色方案。viridis 是一种常用的、颜色渐变平滑的方案</span><br><span class="hljs-comment"># alpha=0.9：alpha 设置了曲面的透明度，0.0 是完全透明，1.0 是完全不透明。</span><br>surf = ax.plot_surface(w_values, b_values, mse_values, cmap=cm.viridis, alpha=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-comment"># 设置图像标题和标签</span><br>ax.set_title(<span class="hljs-string">&quot;3D 损失函数（MSE）表面图&quot;</span>, fontsize=<span class="hljs-number">16</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;w (权重)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;b (截距)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.set_zlabel(<span class="hljs-string">&#x27;MSE (损失)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br><br><span class="hljs-comment"># 添加颜色条</span><br>fig.colorbar(surf, shrink=<span class="hljs-number">0.5</span>, aspect=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># 找到损失函数的最小值点（理论上，w=2.0, b=0.0 时损失为0）</span><br><span class="hljs-comment"># np.argmin()：这个函数返回一个数组中最小值元素的索引</span><br><span class="hljs-comment"># axis=None：忽略数组的维度，而是在整个二维矩阵中寻找唯一的一个最小值。它会返回这个最小值在一维展平（flattened）后的数组中的索引</span><br><span class="hljs-comment"># axis=0：表示沿着列（columns）进行操作，有几列就会返回几个元素的数组</span><br><span class="hljs-comment"># axis=1：表示沿着行（rows）进行操作，有几行就返回几个元素的数组</span><br><span class="hljs-comment"># 例如取None时 mse_values 矩阵是 [[10, 20], [3, 40]]，那么它的最小值是 3</span><br><span class="hljs-comment"># 在一维展平后，数组变为 [10, 20, 3, 40]，3 的索引是 2。所以 np.argmin() 会返回 2</span><br><span class="hljs-comment"># np.unravel_index()：反向将一维数组的索引转换为二维，但是要填入参数 shape：即我们传入的是原始矩阵的形状</span><br><span class="hljs-comment"># 最后返回的是二维坐标 (w, b)</span><br>min_mse_index = np.unravel_index(np.argmin(mse_values, axis=<span class="hljs-literal">None</span>), mse_values.shape)<br><span class="hljs-comment"># 在两个矩阵中获取最小MES对应的具体值</span><br>min_b = b_values[min_mse_index]<br>min_w = w_values[min_mse_index]<br><br><span class="hljs-comment"># 在图上标记最小值点</span><br><span class="hljs-comment"># 参数依次对应：颜色、形状、大小、标签说明</span><br>ax.scatter(min_w, min_b, mse_values.<span class="hljs-built_in">min</span>(), color=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, s=<span class="hljs-number">100</span>,<br>           label=<span class="hljs-string">f&#x27;最小损失点\n(w=<span class="hljs-subst">&#123;min_w:<span class="hljs-number">.2</span>f&#125;</span>, b=<span class="hljs-subst">&#123;min_b:<span class="hljs-number">.2</span>f&#125;</span>)&#x27;</span>)<br><br><span class="hljs-comment"># 显示散点图的图例</span><br>ax.legend()<br><br><span class="hljs-comment"># 调整视角以获得更好的可视化效果</span><br><span class="hljs-comment"># elev 是仰角，azim 是方位角</span><br>ax.view_init(elev=<span class="hljs-number">20</span>, azim=-<span class="hljs-number">120</span>)<br><br><span class="hljs-comment"># 显示最终绘制好的图形窗口</span><br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250917225211998.png" srcset="/img/loading.gif" lazyload alt="image-20250917225211998"></p>
</li>
</ul>
<h2 id="分治法求取多维权重（被舍弃的方法）"><a href="#分治法求取多维权重（被舍弃的方法）" class="headerlink" title="分治法求取多维权重（被舍弃的方法）"></a>分治法求取多维权重（被舍弃的方法）</h2><ul>
<li>当涉及的权重有多个时，以二维为例，假设每个权重取100个值，则一共有100^2种组合（维度的诅咒）</li>
<li>首先取一个稀疏点阵，比如 4 x 4 的稀疏点阵，求取这16个点的MSE值</li>
<li>再选取MSE最小的那个点，在它的周围再次取一个 4 x 4 的稀疏点阵，重复计算</li>
<li>以此试图找到使得MSE最小的权重组合</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<p>弊端：为什么被舍弃</p>
<ul>
<li>机器学习中的优化问题（如线性回归、神经网络训练）通常是<strong>连续可导</strong>的，不适合用分治法分成若干个小问题</li>
</ul>
</blockquote>
<h2 id="梯度下降算法（推荐）"><a href="#梯度下降算法（推荐）" class="headerlink" title="梯度下降算法（推荐）"></a>梯度下降算法（推荐）</h2><ul>
<li>先随机取一个权重组合</li>
<li>计算取该组合时，损失函数在此<strong>对权重</strong>的导数（也叫<strong>梯度</strong>）；<ul>
<li>导数为正，代表往右递增，往左递减</li>
<li>导数为负，代表往右递减，往左递增</li>
</ul>
</li>
<li>以导数为基准，对权重进行迭代，而不是像穷举法那样以相同的步长进行迭代</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918165530078.png" srcset="/img/loading.gif" lazyload alt="一维权重求导过程"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918165604233.png" srcset="/img/loading.gif" lazyload alt="权重迭代公式"></p>
<ul>
<li>权重迭代公式中的 <strong>α</strong> 的名称叫做<strong>学习率</strong>，是一个很重要的参数，相当于穷举法中的步幅<ul>
<li>它不是通过训练学习出来的，而是需要我们在训练前手动设置。选择一个合适的学习率是训练一个高性能模型的关键一步。</li>
<li>步子迈得太大，可能会直接<strong>跳过</strong>损失函数的最小值点，甚至在“山谷”的两侧来回震荡，导致损失值越来越大，永远无法收敛。这就像你下山时，每一步都跨得太大，结果不是跑到山脚下，而是跳到了对面的悬崖上。</li>
<li>步子迈得太小，虽然能够保证每一步都朝着正确的方向前进，但<strong>收敛速度会非常慢</strong>，需要进行大量的迭代才能到达最小值。这会极大地增加训练所需的时间和计算资源。这就像你在下山时，每一步都只挪动一小段距离，要花很长时间才能到达山脚。</li>
</ul>
</li>
</ul>
<h3 id="梯度下降算法的问题"><a href="#梯度下降算法的问题" class="headerlink" title="梯度下降算法的问题"></a>梯度下降算法的问题</h3><ul>
<li>同样是局部最优解，但是比分治法处理的更平滑，也更容易克服，比如我采用多轮次，每轮选取不同的初始点。</li>
<li>鞍点问题：<ul>
<li>一维权重时，也就是驻点，即导数为零的点，会导致迭代停止</li>
<li>多维权重时，可能会形成像马鞍（薯片）一样的曲面，从某个方向看，这个点是<strong>局部最高点</strong>（比如马鞍的前后方向），从另一个方向看，这个点又是<strong>局部最低点</strong>（比如马鞍的左右方向）。</li>
</ul>
</li>
</ul>
<h3 id="简化模型使用梯度下降算法预测权重（无截距）"><a href="#简化模型使用梯度下降算法预测权重（无截距）" class="headerlink" title="简化模型使用梯度下降算法预测权重（无截距）"></a>简化模型使用梯度下降算法预测权重（无截距）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>] <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>] <span class="hljs-comment"># 输出</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x*w<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># MSE损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MSE</span>(<span class="hljs-params">xd, yd, w</span>):<br>    loss_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        loss_sum += loss(x_val, y_val, w)<br>    <span class="hljs-keyword">return</span> loss_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 定义梯度函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">xd, yd, w</span>):<br>    grad_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        grad_sum += <span class="hljs-number">2</span> * x_val * (x_val * w - y_val)<br>    <span class="hljs-keyword">return</span> grad_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 初始随机权重</span><br>w = <span class="hljs-number">1.0</span><br><span class="hljs-comment"># 定义学习率</span><br>a = <span class="hljs-number">0.01</span><br><span class="hljs-comment"># 轮次值列表</span><br>epoch_list = []<br><span class="hljs-comment"># MSE误差列表</span><br>mse_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>,  forward(<span class="hljs-number">4</span>, w))<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    mes_val = MSE(x_data, y_data, w)<br>    mse_list.append(mes_val)<br>    grad_val = gradient(x_data, y_data, w)<br>    w -= a * grad_val<br>    epoch_list.append(epoch)<br>    <span class="hljs-comment"># 详细值</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, w: <span class="hljs-subst">&#123;w&#125;</span>, mes: <span class="hljs-subst">&#123;mes_val&#125;</span>&quot;</span>)<br>    <span class="hljs-comment"># 保留两位小数的值</span><br>    <span class="hljs-comment"># print(f&quot;epoch: &#123;epoch&#125;, w: &#123;round(w, 2)&#125;, mes: &#123;round(mes_val, 2)&#125;&quot;)</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br>plt.plot(epoch_list, mse_list)<br>plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;MES&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918204114132.png" srcset="/img/loading.gif" lazyload alt="image-20250918204114132"></p>
<h3 id="随机梯度下降算法-SGD"><a href="#随机梯度下降算法-SGD" class="headerlink" title="随机梯度下降算法(SGD)"></a>随机梯度下降算法(SGD)</h3><ul>
<li><p>上述的梯度算法，对于每次权重迭代的大小是由<strong>所有数据的损失汇总取平均值得到的误差</strong>来计算的，这样的方式在遇到鞍点时可能会停滞不前</p>
</li>
<li><p>而随机梯度算法，权重的迭代大小是由<strong>随机取一个样本（也可以按照顺序随机，也叫顺序随机梯度算法），用其损失对权重求导</strong>，这样的方式可能可以跨过鞍点，使权重继续前进</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918213038164.png" srcset="/img/loading.gif" lazyload alt="image-20250918213038164"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>]  <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>]  <span class="hljs-comment"># 输出</span><br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># 定义梯度函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">x, y, w</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * x * (x * w - y)<br><br><span class="hljs-comment"># 初始化权重和学习率</span><br>w = <span class="hljs-number">1.0</span><br>a = <span class="hljs-number">0.01</span><br><br><span class="hljs-comment"># 用于记录每个轮次的平均损失</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 训练循环</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-comment"># 在每个轮次开始时，初始化总损失</span><br>    epoch_loss_sum = <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># 随机选择一个样本进行训练</span><br>    index = random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<span class="hljs-comment"># 这里是真随机</span><br>    x_val = x_data[index]<br>    y_val = y_data[index]<br><br>    <span class="hljs-comment"># 计算梯度并更新权重</span><br>    grad = gradient(x_val, y_val, w)<br>    w -= a * grad<br><br>    <span class="hljs-comment"># 计算当前样本的损失并累加到总损失中</span><br>    current_loss = loss(x_val, y_val, w)<br><br>    <span class="hljs-comment"># 因为是SGD，一个轮次只训练一个样本，所以直接记录当前损失即可</span><br>    epoch_loss = current_loss / <span class="hljs-number">1</span> <span class="hljs-comment"># loss的计算只是为了输出体现为0而已</span><br><br>    <span class="hljs-comment"># 打印当前轮次的信息</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, w = <span class="hljs-subst">&#123;w:<span class="hljs-number">.2</span>f&#125;</span>, loss = <span class="hljs-subst">&#123;epoch_loss:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-comment"># 记录每个轮次的平均损失和轮次数，用于绘图</span><br>    epochs_list.append(epoch)<br>    costs_list.append(epoch_loss)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<ul>
<li><p>输出结果（由于每次都取单独一个样本进行计算梯度，所以波动会很大，每次图像基本不一样）</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918221902242.png" srcset="/img/loading.gif" lazyload alt="image-20250918221902242"></p>
</li>
</ul>
<h3 id="小批量梯度下降算法-Mini-BGD"><a href="#小批量梯度下降算法-Mini-BGD" class="headerlink" title="小批量梯度下降算法(Mini-BGD)"></a>小批量梯度下降算法(Mini-BGD)</h3><ul>
<li>现在普遍也称<strong>BGD</strong></li>
<li>比如训练集有8对数据，每个小批次为2，则会分为4个批次batch</li>
<li>梯度计算是取当前批次内的样本所有梯度取平均，再用这个<strong>平均梯度</strong>更新权重</li>
<li>计算并记录当前 <strong>Epoch 的最终性能指标（Cost 或 Loss）</strong>时，都遵循，使用<strong>更新后的权重 w</strong>，对<strong>整个数据集</strong>取 <strong>MSE（或平均损失）</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 假设数据集有8组</span><br>x_data = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>])<br>y_data = np.array([<span class="hljs-number">2.3</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">5.8</span>, <span class="hljs-number">8.4</span>, <span class="hljs-number">10.3</span>, <span class="hljs-number">11.7</span>, <span class="hljs-number">14.5</span>, <span class="hljs-number">15.9</span>])<br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 使用 NumPy 计算整个样本的平均损失</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg_loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> np.mean((y_pred - y) ** <span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 使用 NumPy 计算整个Batch的平均梯度</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg_gradient</span>(<span class="hljs-params">x, y, w</span>):<br>    <span class="hljs-keyword">return</span> np.mean(<span class="hljs-number">2</span> * x * (x * w - y))<br><br><span class="hljs-comment"># 初始化</span><br>w = <span class="hljs-number">1.0</span> <span class="hljs-comment"># 权重</span><br>a = <span class="hljs-number">0.0001</span> <span class="hljs-comment"># 学习率</span><br>batch_size = <span class="hljs-number">2</span> <span class="hljs-comment"># 批次Batch大小</span><br>num_samples = <span class="hljs-built_in">len</span>(x_data) <span class="hljs-comment"># 数据总组数</span><br>num_batches = num_samples // batch_size  <span class="hljs-comment"># 批次Batch数量</span><br><br><span class="hljs-comment"># 轮次列表 与 轮次的平均损失列表</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 训练循环</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>):<br>    <span class="hljs-comment"># 第一步：打乱数据</span><br>    indices = np.random.permutation(num_samples) <span class="hljs-comment"># 生成一个包含从 0 到 num_samples-1 的随机排列数组</span><br>    <span class="hljs-comment"># 每个轮次的 数据顺序都不同</span><br>    shuffled_x = x_data[indices]<br>    shuffled_y = y_data[indices]<br><br>    <span class="hljs-comment"># 第二步：创建批次Batch并循环</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):<br>        <span class="hljs-comment"># 得到当前批次Batch的 起始 和 终止 索引</span><br>        start_index = i * batch_size<br>        end_index = start_index + batch_size<br>        <span class="hljs-comment"># 获取当前批次Batch的数据</span><br>        batch_x = shuffled_x[start_index:end_index]<br>        batch_y = shuffled_y[start_index:end_index]<br><br>        <span class="hljs-comment"># 计算当前批次Batch的平均梯度</span><br>        avg_batch_grad = avg_gradient(batch_x, batch_y, w)<br><br>        <span class="hljs-comment"># 更新权重</span><br>        w -= a * avg_batch_grad<br><br>    <span class="hljs-comment"># 计算并记录当前轮次（epoch）的平均损失</span><br>    avg_epoch_loss = avg_loss(x_data, y_data, w)<br><br>    <span class="hljs-comment"># 记录 当前轮次 和 当前轮次 的平均损失至数组方便绘图</span><br>    epochs_list.append(epoch)<br>    costs_list.append(avg_epoch_loss)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, w = <span class="hljs-subst">&#123;w&#125;</span>, avg_loss = <span class="hljs-subst">&#123;avg_epoch_loss&#125;</span>&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919174947709.png" srcset="/img/loading.gif" lazyload alt="image-20250919174947709"></p>
<ul>
<li>上述代码在求取平均值时采用了**np.mean()**的方法，使代码更简洁。</li>
</ul>
<h1 id="复杂神经网络初体验"><a href="#复杂神经网络初体验" class="headerlink" title="复杂神经网络初体验"></a>复杂神经网络初体验</h1><p>上述所实现的模型都只是很简单的少量权重，少量层数的线性模型。</p>
<p>那考虑如下图的大量权重，大量层数的<strong>多隐层前馈全连接神经网络</strong></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919214751381.png" srcset="/img/loading.gif" lazyload alt="image-20250919214751381"></p>
<ul>
<li>x11 先变成 y11，得通过H1，H2，H3，H4，这四层权重</li>
<li>输入X，是一个 5 * 1 的矩阵，第一层H1，是一个 6 * 1 的矩阵</li>
<li>而矩阵X变换成矩阵H1，需要权重矩阵 W1 与 X 相乘，即 W * X &#x3D; H</li>
<li>也就是说 W1 是一个 6 * 5 的矩阵</li>
<li>即 [6 * 5] * [5, 1] &#x3D; [6 * 1]</li>
<li>同理 H1 变换至 H2，需要一个 [7 * 6] 的W2矩阵</li>
</ul>
<h1 id="反向梯度传播算法"><a href="#反向梯度传播算法" class="headerlink" title="反向梯度传播算法"></a>反向梯度传播算法</h1><p>上述的全连接神经网络，有好几百个权重，如果每个都采用先写解析式是很困难的，而且每层之间都是复合函数，这是个相当复杂的工作量。</p>
<p><strong>故此考虑，把这样一个复杂的神经网络，看作一个图，通过图来传播梯度，根据链式法则求取梯度，这就叫反向梯度传播算法。</strong></p>
<h2 id="两层简单图"><a href="#两层简单图" class="headerlink" title="两层简单图"></a>两层简单图</h2><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919221832160.png" srcset="/img/loading.gif" lazyload alt="image-20250919221832160"></p>
<ul>
<li><p>每一层的结构都是一样的</p>
</li>
<li><p>权重矩阵 W 与输入矩阵相乘，得到一个新的矩阵</p>
</li>
<li><p>新的矩阵与偏置值矩阵 b 相加，得到输出结果</p>
</li>
<li><p><strong>当前层的输出作为下一层的输入</strong></p>
</li>
<li><p>最终得到预测解析式，类似于迭代套娃</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919222136045.png" srcset="/img/loading.gif" lazyload alt="image-20250919222136045"></p>
</li>
<li><p>但是这样建立模型会有一个<strong>问题</strong>，因为不管你套娃多少层，都可以通过计算展开成单层模型的样子（又回到最初的起点），不管多少层都变成单一线性的，如下图</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919222551620.png" srcset="/img/loading.gif" lazyload alt="image-20250919222551620"></p>
</li>
</ul>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><ul>
<li><p>解决方法也很简单，即在每一层的输入之前，对上一层的输出加上一个非线性的变化函数，整个函数也叫做<strong>激活函数</strong>，这样就不能对迭代式子进行展开了</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919222858405.png" srcset="/img/loading.gif" lazyload alt="image-20250919222858405"></p>
</li>
</ul>
<h2 id="手算最小简单图"><a href="#手算最小简单图" class="headerlink" title="手算最小简单图"></a>手算最小简单图</h2><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/3c740378c7b44b705869e7192572877.jpg" srcset="/img/loading.gif" lazyload alt="3c740378c7b44b705869e7192572877"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/bd0537a18fae60f6476a65d7d9e471c.jpg" srcset="/img/loading.gif" lazyload alt="bd0537a18fae60f6476a65d7d9e471c"></p>
<h2 id="Tensor的作用"><a href="#Tensor的作用" class="headerlink" title="Tensor的作用"></a>Tensor的作用</h2><p>Tensor 是一个多维数组，可以用来存储标量（0 维）、向量（1 维）、矩阵（2 维）乃至更高维度的数据。</p>
<ul>
<li><strong>存储所有数据类型</strong>：在 PyTorch 中，无论是模型的输入数据（如图像的像素值、文本的词向量），模型的参数（权重 W 和偏置 b），还是中间计算结果，<strong>一切皆为 Tensor</strong>。</li>
</ul>
<p>Tensor是 PyTorch 框架中<strong>动态计算图</strong>的基石，负责存储数据和梯度</p>
<ul>
<li>Tensor 是构建计算图的基本单元。在一个深度学习模型中，所有运算（如加、乘、矩阵乘法等）都是 Tensor 之间的操作<ul>
<li><strong>动态 (Dynamic)<strong>：这是 PyTorch 的关键特性。计算图是</strong>在运行时</strong>（即每次前向传播时）动态构建的。这意味着图的结构可以根据输入数据的不同或程序逻辑（如条件判断）而改变。</li>
</ul>
</li>
<li>Tensor 内部存储了两个至关重要的数值<ul>
<li>数据 (Data)：主体部分，即<strong>节点的值</strong>，用于存储实际的输入数据、模型参数（权重 W、b）以及前向传播过程中的所有中间结果。</li>
<li>梯度 (Grad)：Tensor 的 <code>.grad</code> 属性，存储了<strong>梯度值 (gradient)<strong>，这个值代表了</strong>损失函数 (loss)</strong> 对该 Tensor 的<strong>导数</strong></li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250924205707890.png" srcset="/img/loading.gif" lazyload alt="image-20250924205707890"></p>
<h2 id="使用Tensor实现反向传播"><a href="#使用Tensor实现反向传播" class="headerlink" title="使用Tensor实现反向传播"></a>使用Tensor实现反向传播</h2><p>关键代码详解：</p>
<ul>
<li><code>loss_val.backward()</code> 执行后，具体哪些对象及其值会发生改变？<ul>
<li>唯一会发生<strong>实质性改变</strong>的对象是<strong>具有 <code>requires_grad=True</code> 属性的叶子 Tensor</strong>，在本例中就是您的权重 w</li>
<li>w.grad：值被累加</li>
<li>w：值保持不变（w的值默认表示data）</li>
<li>其他中间 Tensor：值保持不变（<code>forward()</code> 过程中产生的中间 Tensor（如 <code>y_pred</code>）的值保持不变，它们所包含的梯度历史信息会被用于反向传播，然后通常会被释放。）</li>
</ul>
</li>
<li>为什么要使用 <strong>with torch.no_grad():</strong> 包裹更新权重的代码？<ul>
<li>防止构建计算图：梯度下降的目标是<strong>修改</strong>权重 <code>w</code> 的值，而不是将这个修改过程作为一次可微分的计算。<code>torch.no_grad()</code> 告诉 PyTorch：“以下操作只是数据管理，不要追踪。”</li>
<li>避免循环依赖和错误：如果没有 <code>no_grad()</code>，PyTorch 会将 <code>w_new = w_old - a * w.grad</code> 这个操作记录到计算图中。</li>
</ul>
</li>
<li>为什么梯度要使用 <code>w.grad.zero_()</code> 置零，以及为什么它不用被 <code>torch.no_grad()</code> 包裹？<ul>
<li><code>backward()</code> 方法的机制是<strong>累加梯度</strong>，除非确实需要累加，否则要显示的使用.zero_()置零</li>
<li><code>w.grad</code> 存储的是<strong>上一次计算的结果</strong>，它是一个<strong>非活跃的 Tensor</strong>。我们并不需要对梯度本身求梯度。PyTorch 已经明确地将对 <code>.grad</code> 属性执行的原地操作（如 <code>zero_()</code>）视为<strong>纯粹的数据管理</strong>，并默认允许它绕过梯度追踪系统。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], dtype=torch.float32)<br>y_data = torch.tensor([<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>], dtype=torch.float32)<br>a = <span class="hljs-number">0.01</span><br><br><span class="hljs-comment"># 创建一个权重tensor，将值用[]框起来</span><br>w = torch.tensor([<span class="hljs-number">1.0</span>])<br><span class="hljs-comment"># 启动自动计算loss对w的梯度，默认关闭</span><br>w.requires_grad_(<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-comment"># 这里刚开始 w 是个tensor，x 是个数</span><br>    <span class="hljs-comment"># 当他俩相乘时，乘号*会自动重载，把x也化为一个tensor对象</span><br>    <span class="hljs-comment"># 最终做乘法返回一个tensor</span><br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 每调用一次loss函数，就动态的构建了计算图</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y</span>):<br>    y_pred = forward(x)<br>    <span class="hljs-keyword">return</span> (y_pred - y) ** <span class="hljs-number">2</span><br><br><span class="hljs-comment"># 轮次列表 与 轮次的平均损失列表</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>,  forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(x_data, y_data):<br>        loss_val = loss(x_val, y_val)<br>        loss_val.backward() <span class="hljs-comment"># 将梯度反向传播至w的tensor，并且与该样本相关的计算图就会被释放</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x = <span class="hljs-subst">&#123;x_val&#125;</span>, y = <span class="hljs-subst">&#123;y_val&#125;</span>, w = <span class="hljs-subst">&#123;w.item()&#125;</span>, grad = <span class="hljs-subst">&#123;w.grad.item():<span class="hljs-number">.4</span>f&#125;</span>, loss = <span class="hljs-subst">&#123;loss_val.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            w -= a * w.grad <span class="hljs-comment"># 使用no_grad()包裹，使tensor只更新data值，不生成计算图</span><br><br>        w.grad.zero_() <span class="hljs-comment"># 更新完成后，将梯度置零，防止累加</span><br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># 计算整个数据集的均方误差 (MSE)</span><br>        final_epoch_loss = torch.mean(loss(x_data, y_data)).item()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;------------------ Epoch <span class="hljs-subst">&#123;epoch:03d&#125;</span> End ------------------&quot;</span>)<br>    epochs_list.append(epoch)<br>    costs_list.append(final_epoch_loss)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250924221417266.png" srcset="/img/loading.gif" lazyload alt="image-20250924221417266"></p>
<h2 id="课后作业：二维权重计算题手算与代码实现"><a href="#课后作业：二维权重计算题手算与代码实现" class="headerlink" title="课后作业：二维权重计算题手算与代码实现"></a>课后作业：二维权重计算题手算与代码实现</h2><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/2d4c3d1409871e726d1c69164a19260.jpg" srcset="/img/loading.gif" lazyload alt="2d4c3d1409871e726d1c69164a19260"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], dtype=torch.float32)<br>y_data = torch.tensor([<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>], dtype=torch.float32)<br>a = <span class="hljs-number">0.01</span><br><br>w1 = torch.tensor([<span class="hljs-number">1.0</span>])<br>w2 = torch.tensor([<span class="hljs-number">1.0</span>])<br>b = torch.tensor([<span class="hljs-number">1.0</span>])<br>w1.requires_grad_(<span class="hljs-literal">True</span>)<br>w2.requires_grad_(<span class="hljs-literal">True</span>)<br>b.requires_grad_(<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> w1 * x ** <span class="hljs-number">2</span> + w2 * x + b<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y</span>):<br>    y_pred = forward(x)<br>    <span class="hljs-keyword">return</span> (y_pred - y) ** <span class="hljs-number">2</span><br><br><span class="hljs-comment"># 轮次列表 与 轮次的平均损失列表</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>,  forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5000</span>):<br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(x_data, y_data):<br>        loss_val = loss(x_val, y_val)<br>        loss_val.backward()<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            w1 -= a * w1.grad<br>            w2 -= a * w2.grad<br>            b -= a * b.grad<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x = <span class="hljs-subst">&#123;x_val&#125;</span>, y = <span class="hljs-subst">&#123;y_val&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;w1 = <span class="hljs-subst">&#123;w1.item():<span class="hljs-number">.3</span>f&#125;</span>, w1&#x27;s new grad = <span class="hljs-subst">&#123;w1.grad.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;w2 = <span class="hljs-subst">&#123;w2.item():<span class="hljs-number">.3</span>f&#125;</span>, w2&#x27;s new grad = <span class="hljs-subst">&#123;w2.grad.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;b = <span class="hljs-subst">&#123;b.item():<span class="hljs-number">.3</span>f&#125;</span>, b&#x27;s new grad = <span class="hljs-subst">&#123;b.grad.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;single_loss = <span class="hljs-subst">&#123;loss_val.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span>)<br><br>        w1.grad.zero_()<br>        w2.grad.zero_()<br>        b.grad.zero_()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># 计算整个数据集的均方误差 (MSE)</span><br>        final_epoch_loss = torch.mean(loss(x_data, y_data)).item()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;------------ Epoch <span class="hljs-subst">&#123;epoch:03d&#125;</span> End ----------- Epoch&#x27;s MSE: <span class="hljs-subst">&#123;final_epoch_loss:<span class="hljs-number">.4</span>f&#125;</span>-----------&quot;</span>)<br>    epochs_list.append(epoch)<br>    costs_list.append(final_epoch_loss)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250925222226179.png" srcset="/img/loading.gif" lazyload alt="image-20250925222226179"></p>
<h2 id="阶段小结"><a href="#阶段小结" class="headerlink" title="阶段小结"></a>阶段小结</h2><h3 id="SGD与Mini-BGD、BGD的区别"><a href="#SGD与Mini-BGD、BGD的区别" class="headerlink" title="SGD与Mini-BGD、BGD的区别"></a>SGD与Mini-BGD、BGD的区别</h3><ul>
<li><p>在 <strong>Mini-batch 梯度下降 (MBGD)</strong> 中，<strong>梯度计算</strong>和 <strong>权重更新</strong> 都是基于<strong>当前批次</strong>的<strong>平均值</strong>。</p>
</li>
<li><p>SGD 的标准定义是：在每一次权重更新时，只使用<strong>一个</strong>样本的梯度。就算他是每个都算，并且按顺序算，但是他更新权重的时机是在单一样本后的。</p>
<ul>
<li>一个样本更新一次w</li>
</ul>
</li>
<li><p>BGD（Batch Gradient Descent，批量梯度下降）的定义是：在进行一次权重更新时，必须使用<strong>整个数据集</strong>的平均梯度。</p>
<ul>
<li>N个样本更新一次w</li>
</ul>
</li>
<li><p>Mini-BGD，<strong>梯度计算</strong>和 <strong>权重更新</strong> 都是基于<strong>当前批次</strong>的<strong>平均值</strong></p>
</li>
</ul>
<h3 id="计算当前Epoch的时机"><a href="#计算当前Epoch的时机" class="headerlink" title="计算当前Epoch的时机"></a>计算当前Epoch的时机</h3><ul>
<li>不管采用哪种梯度下降算法（BGD、MBGD、SGD），计算并记录当前 <strong>Epoch 的最终性能指标（Cost 或 Loss）</strong>时，都遵循，使用<strong>更新后的权重 w</strong>，对<strong>整个数据集</strong>取 <strong>MSE（或平均损失）</strong></li>
</ul>
<h1 id="使用PyTorch实现线性回归"><a href="#使用PyTorch实现线性回归" class="headerlink" title="使用PyTorch实现线性回归"></a>使用PyTorch实现线性回归</h1><h2 id="名词解释：张量、标量、向量、矩阵"><a href="#名词解释：张量、标量、向量、矩阵" class="headerlink" title="名词解释：张量、标量、向量、矩阵"></a>名词解释：张量、标量、向量、矩阵</h2><ul>
<li><p>张量：即前面提到的Tensor，是 PyTorch 存储数据和执行计算的基本单位，张量是一个<strong>多维数组</strong>，它是标量、向量和矩阵的广义统称。</p>
</li>
<li><p>标量：标量是一个**零维张量 (0-D Tensor)**，它只包含一个数值，没有方向或大小之分。</p>
<ul>
<li>&#96;&#96;&#96;<br>torch.tensor(5.0)<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><br><span class="hljs-bullet">-</span> 向量：向量是一个<span class="hljs-strong">**一维张量 (1-D Tensor)**</span>，它包含一列有序的数值，只有大小和方向。<br><br><span class="hljs-bullet">  -</span> <span class="hljs-code">```</span><br><span class="hljs-code">    torch.tensor([1, 2, 3])</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>矩阵：矩阵是一个**二维张量 (2-D Tensor)**，它由行和列组成，是两个向量的排列。</p>
<ul>
<li>&#96;&#96;&#96;<br>torch.tensor([[1, 2], [3, 4]])	<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><code class="hljs less"><br>&gt; <span class="hljs-selector-attr">[!TIP]</span><br>&gt;<br>&gt; 几个 <span class="hljs-selector-attr">[]</span> 就是几维张量<br><br>## 前置关键代码解释<br><br>**<span class="hljs-selector-tag">nn</span><span class="hljs-selector-class">.Linear</span>(in_features, out_features, bias=true)**<br><br>一共三个主要参数需要填写，<span class="hljs-selector-tag">Linear</span>源码如下图：<br><br>!<span class="hljs-selector-attr">[image-20251011191525307]</span>(<span class="hljs-attribute">https</span>:<span class="hljs-comment">//gitee.com/Gu-taicheng/image/raw/master/img/image-20251011191525307.png)</span><br><br>- **in_features**：输入特征数，或者说输入的维度，比如说你的输入矩阵是一个**<span class="hljs-number">3</span>*<span class="hljs-number">2</span>**的**矩阵**，每一行代表单个样本，也就是说一个样本有两个维度，即列数，那么这里就应该设置**in_features=<span class="hljs-number">2</span>**<br>- **out_features**：输出特征数，同上。如果输出是一个**<span class="hljs-number">4</span>*<span class="hljs-number">3</span>**的矩阵，每一行代表单个样本，也就是有<span class="hljs-number">3</span>个维度，那么就应该设置out_features=**<span class="hljs-number">3</span>**<br>- **bias**：默认为**True**，用于是否要在计算过程中加上偏置项b<br><br>&gt; [!NOTE]<br>&gt;<br>&gt; 这里有一个很重要的思想转变，即输入输出的值，**从<span class="hljs-string">&quot;数字&quot;</span>变成“矩阵”**<br>&gt;<br>&gt; 看网课时我一开始一直没理解老师为什么要一直使用矩阵的知识，我默认认为一个线性模型y = w*x + b中的x、y、w、b都只是一个数字。<br>&gt;<br>&gt; 但是现在要转变观念，X可以是一个m * n的矩阵，Y可以是一个 m * q 的矩阵<br>&gt;<br>&gt; <br>&gt;<br>&gt; 那么又引申出一个问题，W应该是怎样的矩阵？<br>&gt;<br>&gt; 当使用PyTorch进行线性回归时，将会遵循以下乘法规则：<br>&gt;<br>&gt; ![image-<span class="hljs-number">20251011193345138</span>](<span class="hljs-attribute">https</span>:<span class="hljs-comment">//gitee.com/Gu-taicheng/image/raw/master/img/image-20251011193345138.png)</span><br><br>至于为什么要对W进行转置，请往下看：<br><br>- 当你输入nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">3</span>，<span class="hljs-number">2</span>)时，也就意味着你的输入数据集X_Data是形状是 **N * <span class="hljs-number">3</span>**，输出数据集Y_Data的形状是 N * <span class="hljs-number">2</span>，总共有**N**个样本<br><br>- 如果采用上图的乘法顺序，W的性质应该刚好是 <span class="hljs-number">3</span> * <span class="hljs-number">2</span>，和你括号中输入的顺序是一致的，但是由于PyTorch内部规定了需要取转置后再乘（如下图），所以为了符合直觉，源代码中特地反转了。<br><br>  ![image-<span class="hljs-number">20251011194135976</span>](<span class="hljs-attribute">https</span>:<span class="hljs-comment">//gitee.com/Gu-taicheng/image/raw/master/img/image-20251011194135976.png)</span><br><br><br><br>**optimizer = torch.optim.<span class="hljs-built_in">SGD</span>(my_model.<span class="hljs-built_in">parameters</span>(), lr=<span class="hljs-number">0.01</span>)**<br><br>- <span class="hljs-built_in">`my_model.parameters()`</span> 的作用是返回一个**迭代器 (iterator)**，其中包含了 <span class="hljs-built_in">`my_model`</span> 实例中所有**需要学习和更新的参数 (Parameter)**。<br>- 反正是pytorch帮忙找，用迭代找的，后续需要再深入学习。<br><br>## 具体代码<br><br><span class="hljs-built_in">``</span>`python<br>import torch<br>from torch import nn ## nn 是 neural network 神经网络的缩写<br>import matplotlib.pyplot as plt<br><br>#定义数据集，设置为矩阵模式，<span class="hljs-number">3</span>*<span class="hljs-number">1</span> 的矩阵<br>x_data = torch.<span class="hljs-built_in">tensor</span>([[<span class="hljs-number">1.0</span>], [<span class="hljs-number">2.0</span>], [<span class="hljs-number">3.0</span>]], dtype=torch.float)<br>y_data = torch.<span class="hljs-built_in">tensor</span>([[<span class="hljs-number">2.0</span>], [<span class="hljs-number">4.0</span>], [<span class="hljs-number">6.0</span>]], dtype=torch.float)<br><br>class <span class="hljs-built_in">LinearRegression</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self):<br>        # 调用父类的初始化方法<br>        <span class="hljs-built_in">super</span>(LinearRegression, self).<span class="hljs-built_in">__init__</span>()<br>        # nn.Linear 自动创建并初始化 w 和 b，反正会自动，具体可以后续详细了解<br>        self.linear = nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>    # 前馈<br>    def <span class="hljs-built_in">forward</span>(self, x):<br>        # 默认执行 y = xW^T + b.<br>        return self.<span class="hljs-built_in">linear</span>(x)<br><br>    # 反馈，PyTorch会自动帮你求导，除非你自己写的效率比PyTorch还高<br><br># 实例化自己的模型<br>my_model = <span class="hljs-built_in">LinearRegression</span>()<br># 均方误差损失函数<br>criterion = nn.<span class="hljs-built_in">MSELoss</span>()<br># 使用随机梯度下降优化器（即更新权重的算法）<br>optimizer = torch.optim.<span class="hljs-built_in">SGD</span>(my_model.<span class="hljs-built_in">parameters</span>(), lr=<span class="hljs-number">0.01</span>)<br><br># 打印训练前的参数<br>w_init = my_model.linear.weight.<span class="hljs-built_in">item</span>()<br>b_init = my_model.linear.bias.<span class="hljs-built_in">item</span>()<br><span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;训练前参数: w=&#123;w_init:.4f&#125;, b=&#123;b_init:.4f&#125;&quot;</span>)<br><br># <span class="hljs-number">3</span>. 训练参数<br>num_epochs = <span class="hljs-number">500</span><br>costs_list = []<br>epochs_list = []<br><br>for epoch in <span class="hljs-built_in">range</span>(num_epochs):<br>    # 每次迭代都需要执行的操作：<br><br>    # (A) 梯度清零：使用优化器内置的方法，替代 w.grad.<span class="hljs-built_in">zero_</span>()<br>    optimizer.<span class="hljs-built_in">zero_grad</span>()<br><br>    # (B) 前向传播<br>    y_pred = <span class="hljs-built_in">my_model</span>(x_data)<br><br>    # (C) 计算损失<br>    loss = <span class="hljs-built_in">criterion</span>(y_pred, y_data)<br><br>    # (D) 反向传播：计算梯度<br>    loss.<span class="hljs-built_in">backward</span>()<br><br>    # (E) 更新权重：使用优化器内置的方法，替代 with torch.<span class="hljs-built_in">no_grad</span>(): W -= lr * W.grad<br>    optimizer.<span class="hljs-built_in">step</span>()<br><br>    # 记录损失（使用更新后的权重计算）<br>    # 如果不在意小精度，也可以直接costs_list.<span class="hljs-built_in">append</span>(loss.<span class="hljs-built_in">item</span>())，也就是使用旧的W<br>    with torch.<span class="hljs-built_in">no_grad</span>():<br>        costs_list.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">criterion</span>(<span class="hljs-built_in">my_model</span>(x_data), y_data).<span class="hljs-built_in">item</span>())<br>    epochs_list.<span class="hljs-built_in">append</span>(epoch)<br><br>    if (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>        current_w = my_model.linear.weight.<span class="hljs-built_in">item</span>()<br>        current_b = my_model.linear.bias.<span class="hljs-built_in">item</span>()<br>        <span class="hljs-built_in">print</span>(f<span class="hljs-string">&#x27;Epoch [&#123;epoch + 1&#125;/&#123;num_epochs&#125;], Loss: &#123;loss.item():.6f&#125;, w: &#123;current_w:.4f&#125;, b: &#123;current_b:.4f&#125;&#x27;</span>)<br><br># <span class="hljs-number">5</span>. 最终预测和结果<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">40</span>)<br>final_w = my_model.linear.weight.<span class="hljs-built_in">item</span>()<br>final_b = my_model.linear.bias.<span class="hljs-built_in">item</span>()<br><br># 预测 x=<span class="hljs-number">4</span><br>x_test = torch.<span class="hljs-built_in">tensor</span>([[<span class="hljs-number">4.0</span>]], dtype=torch.float32)<br>y_test_pred = <span class="hljs-built_in">my_model</span>(x_test).<span class="hljs-built_in">item</span>()<br><br><span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;训练后参数: w=&#123;final_w:.4f&#125;, b=&#123;final_b:.4f&#125;&quot;</span>)<br><span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;训练后预测 x=4, y_pred=&#123;y_test_pred:.4f&#125;&quot;</span>)<br><br># <span class="hljs-number">6</span>. 绘图<br>plt.<span class="hljs-built_in">plot</span>(epochs_list, costs_list)<br>plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">&#x27;MSE Loss&#x27;</span>)<br>plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.<span class="hljs-built_in">show</span>()<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011200926371.png" srcset="/img/loading.gif" lazyload alt="训练结果最好的一次"></p>
<h3 id="使用其他优化器"><a href="#使用其他优化器" class="headerlink" title="使用其他优化器"></a>使用其他优化器</h3><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011202737953.png" srcset="/img/loading.gif" lazyload alt="image-20251011202737953"></p>
<h1 id="逻辑斯蒂回归（Logistic-Regression）"><a href="#逻辑斯蒂回归（Logistic-Regression）" class="headerlink" title="逻辑斯蒂回归（Logistic Regression）"></a>逻辑斯蒂回归（Logistic Regression）</h1><p>逻辑斯蒂回归本质上是一个<strong>二分类</strong>算法，并不是像线性回归那样输出一个具体的值，而是输出该样本属于某个分类的<strong>概率</strong></p>
<ul>
<li><p>例如手写的0~9的数据集，难道要拿到一张手写图，直接输出这是具体的几吗？并不是，而是输出它是0的概率P(0)，它是1的概率P(1)……，最终选择概率最大的</p>
</li>
<li><p>而逻辑斯蒂回归主要解决的是二分类，也就是只有两个分类。</p>
</li>
<li><p>由于概率总和肯定等于1，所以我们只需要计算其中一个分类的概率即可。</p>
</li>
<li><p>而逻辑斯蒂回归的核心在于将线性回归的结果通过一个<strong>非线性函数</strong>——<strong>Sigmoid 函数（或 Logit 函数）</strong>——映射到 (0,1) 的概率区间。<strong>所以要先进行线性回归</strong></p>
</li>
<li><p>其中最经典的函数是<strong>Logit 函数</strong>（如下图），它完美的把整个实数域限制在了[-1, 1]之间，当然还有很多其他的<strong>Sigmoid 函数</strong>，但是由于<strong>Logit 函数</strong>太过经典，<strong>所以一般说Sigmoid 函数时，往往代指的就是Logit 函数</strong></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011213706761.png" srcset="/img/loading.gif" lazyload alt="最经典的Logit函数"></p>
</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011213740427.png" srcset="/img/loading.gif" lazyload alt="其他的Sigmoid函数"></p>
<ul>
<li>与线性回归对比，回顾一下前面提到的<strong>激活函数</strong>，以及为什么需要激活函数</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011213958565.png" srcset="/img/loading.gif" lazyload alt="image-20251011213958565"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011214036393.png" srcset="/img/loading.gif" lazyload alt="image-20251011214036393"></p>
<ul>
<li><p>损失函数的改变（虽然PyTorch里面有现成的，不懂也没关系）</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011214218535.png" srcset="/img/loading.gif" lazyload alt="image-20251011214218535"></p>
</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Pytorch-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#Pytorch - 深度学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Pytorch深度学习实践</div>
      <div>https://blog.gutaicheng.top/2025/09/16/Pytorch深度学习实践/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>GuTaicheng</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年9月16日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/09/16/Conda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/" title="Conda使用教程">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Conda使用教程</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/09/10/Pytorch%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" title="Pytorch环境安装教程">
                        <span class="hidden-mobile">Pytorch环境安装教程</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"Mm7zXkSdUNoJGtCjXx5EokSc-gzGzoHsz","appKey":"QeD6ibBU3GKctfSty5fFG9Xz","path":"window.location.pathname","placeholder":"什么都不用填就可以评论啦(当然留个qq或者邮箱更好哦)","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
