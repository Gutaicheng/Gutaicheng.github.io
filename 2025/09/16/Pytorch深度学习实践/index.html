

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tubiao2.png">
  <link rel="icon" href="/img/tubiao2.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GuTaicheng">
  <meta name="keywords" content="博客, 学习, Java">
  
    <meta name="description" content="本博客是根据Bilibil《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili所做的笔记 名词解释 数据集(Data Set)  训练集(Traing Set)  已知输入x和输出y，用于训练模型  训练集可在分一部分出来作为开发集，用于评估模型的泛化能力，防止过拟合    测试集(Test Set)  只知道输入x     过拟合(Overfitting)  是模型在训练集">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch深度学习实践">
<meta property="og:url" content="https://blog.gutaicheng.top/2025/09/16/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="GuTaicheng&#39;s Blog">
<meta property="og:description" content="本博客是根据Bilibil《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili所做的笔记 名词解释 数据集(Data Set)  训练集(Traing Set)  已知输入x和输出y，用于训练模型  训练集可在分一部分出来作为开发集，用于评估模型的泛化能力，防止过拟合    测试集(Test Set)  只知道输入x     过拟合(Overfitting)  是模型在训练集">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250910122424933.png">
<meta property="article:published_time" content="2025-09-16T13:50:00.000Z">
<meta property="article:modified_time" content="2025-10-22T14:44:11.961Z">
<meta property="article:author" content="GuTaicheng">
<meta property="article:tag" content="Pytorch - 深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250910122424933.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Pytorch深度学习实践 - GuTaicheng&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.gutaicheng.top","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":21404155,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    <!-- 51.la Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('//js.users.51.la/21404155.js');
      }
    </script>
  

  

  



  
  <meta name="referrer" content="never">
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GuTaicheng</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Pytorch深度学习实践"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-09-16 21:50" pubdate>
          2025年9月16日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          30k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          251 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Pytorch深度学习实践</h1>
            
            
              <div class="markdown-body">
                
                <meta name="referrer" content="no-referrer" />



<p>本博客是根据Bilibil<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Y7411d7Ys/">《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili</a>所做的笔记</p>
<h1 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1><ul>
<li><p>数据集(Data Set)</p>
<ul>
<li><p>训练集(Traing Set)</p>
<ul>
<li><p>已知<strong>输入x</strong>和<strong>输出y</strong>，用于训练模型</p>
</li>
<li><p>训练集可在分一部分出来作为<strong>开发集</strong>，用于评估模型的泛化能力，防止过拟合</p>
</li>
</ul>
</li>
<li><p>测试集(Test Set)</p>
<ul>
<li>只知道<strong>输入x</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>过拟合(Overfitting)</p>
<ul>
<li>是模型在训练集上表现很好，但在新数据上表现很差</li>
<li>但它并没有真正学到数据背后的普遍规律，而是把训练数据中的<strong>所有特征，包括噪声</strong>，都当作了学习目标。当它遇到<strong>新数据</strong>时，它的表现会急剧下降，预测结果非常不准确</li>
</ul>
</li>
<li><p>泛化(Generalization)</p>
<ul>
<li>泛化是机器学习模型的真正目标，它是指模型在<strong>未见过的新数据</strong>上的表现能力</li>
</ul>
</li>
</ul>
<h1 id="线性模型-Linear-Model"><a href="#线性模型-Linear-Model" class="headerlink" title="线性模型(Linear Model)"></a>线性模型(Linear Model)</h1><p>拿到数据集，先试着用线性模型<strong>试一下</strong>，即找到一个<strong>权重w和截距b</strong>，使得训练集满足：</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915205727712.png" srcset="/img/loading.gif" lazyload alt="image-20250915205727712"></p>
<ul>
<li><p><strong>ŷ</strong>读作y_hat，表示预测的值，并不是准确值</p>
</li>
<li><p>为了方便学习，这里选择简化模型，将截距b去掉</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915210137135.png" srcset="/img/loading.gif" lazyload alt="image-20250915210137135"></p>
</li>
</ul>
<p>所以线性模型的关键是找到一个合适的<strong>权重w</strong>，而什么叫做”合适呢“？即误差小。</p>
<ul>
<li><p>初始数据集可能并不是严格的在直线上的点集，会是离散的，而我们预设的线性模型是一条严格的直线</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915211004528.png" srcset="/img/loading.gif" lazyload alt="image-20250915211004528"></p>
</li>
<li><p>而预测的ŷ(a)会和对应的同一个横坐标a的真实值y(a)有差值</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915211334561.png" srcset="/img/loading.gif" lazyload alt="image-20250915211334561"></p>
</li>
<li><p>但是由于差值有正有负，所以对差值进行<strong>求平方</strong>（当然也有其他方法，取绝对值等，<strong>但是平方对于误差大的惩罚越大</strong>）即可消去负值影响，而这些平方值即为<strong>损失(loss)</strong></p>
<ul>
<li>loss只是针对单独一个样本的</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915212029351.png" srcset="/img/loading.gif" lazyload alt="image-20250915212029351"></p>
</li>
<li><p>得到单独样本<strong>损失</strong>后，对所有样本的损失进行<strong>求和</strong>，再取<strong>平均值</strong>，即得到训练集的<strong>误差(Error)</strong></p>
<ul>
<li>采用平方来计算这种方法得到的误差叫做<strong>平均平方误差</strong>或者<strong>均方误差</strong>（Mean Squared Error, <strong>MSE</strong>）</li>
<li>误差(Error)是针对训练集training set的</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915213243814.png" srcset="/img/loading.gif" lazyload alt="image-20250915213243814"></p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>注意：<br>Loss Function (损失函数)是衡量模型在<strong>单个数据点</strong>上的表现有多差，比如上述差值的平方</p>
<p>Cost Function (成本函数 或 目标函数)是衡量模型在<strong>整个数据集</strong>上的总体表现有多差，比如MSE</p>
</blockquote>
<blockquote>
<p>虽然技术上有区别，但在实际的工程对话中，人们经常用 <strong>Loss</strong> 来泛指<strong>整个优化过程中的误差指标</strong>（也就是成本函数 Cost），例如：“我们看一下训练 Loss 是多少。”</p>
</blockquote>
</li>
<li><p>最终找到一个误差最小的<strong>权重w</strong>，就是线性模型的关键。</p>
<p>比如下图的数据集与权重选择</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915213736813.png" srcset="/img/loading.gif" lazyload alt="image-20250915213736813"></p>
</li>
</ul>
<h2 id="穷举法找权重w"><a href="#穷举法找权重w" class="headerlink" title="穷举法找权重w"></a>穷举法找权重w</h2><ul>
<li><p>先随机找一个<strong>大的步长</strong>范围，尝试找到一个<strong>可能（可能有增或减的趋势）</strong>存在最小权重的范围</p>
</li>
<li><p>再在这个范围内，<strong>缩短步长</strong>，以此找到产生最小误差的权重</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250915214623127.png" srcset="/img/loading.gif" lazyload alt="image-20250915214623127"></p>
</li>
<li><p>代码演示，绘制图如上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>] <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>] <span class="hljs-comment"># 输出</span><br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># MES损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MSE</span>(<span class="hljs-params">xd, yd, w</span>):<br>    loss_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        loss_sum += loss(x_val, y_val, w)<br>    <span class="hljs-keyword">return</span> loss_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 权重值列表</span><br>w_list = []<br><span class="hljs-comment"># MSE误差列表</span><br>mse_list = []<br><br><span class="hljs-comment"># w权重取值范围在 [0.0, 4.0], 步长间隔为 0.1</span><br><span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0.0</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">0.1</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w=&#x27;</span>, w)<br>    mes = MSE(x_data, y_data, w)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;MSE=&#x27;</span>, mes)<br>    w_list.append(w)<br>    mse_list.append(mes)<br><br>plt.plot(w_list, mse_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;w&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
</li>
<li><p>非简化版本，基本模型：<strong>ŷ &#x3D; x*w + b</strong>;   w，b，mse，三维绘制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> cm<br><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D  <span class="hljs-comment"># 导入3D绘图模块</span><br><br><span class="hljs-comment"># --- 添加这三行，设置中文显示 ---</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]  <span class="hljs-comment"># 指定默认字体为黑体</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span>     <span class="hljs-comment"># 解决保存图像时负号 &#x27;-&#x27; 显示为方块的问题</span><br><span class="hljs-comment"># --------------------------------</span><br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>] <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>] <span class="hljs-comment"># 输出</span><br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-keyword">return</span> x * w + b<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w, b</span>):<br>    y_pred = forward(x, w, b)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># MES损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MSE</span>(<span class="hljs-params">xd, yd, w, b</span>):<br>    loss_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        loss_val = loss(x_val, y_val, w, b)<br>        loss_sum += loss_val<br>    <span class="hljs-keyword">return</span> loss_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 定义 w 和 b 的取值范围</span><br><span class="hljs-comment"># arange是 Numpy 库中的一个函数，它类似于 Python 内置的 range() 函数，但主要用于创建包含浮点数的数组。</span><br><span class="hljs-comment"># 第一行代码会创建一个从 0.0 开始，到 4.1 结束（不包含 4.1），步长为 0.1 的一维数组。</span><br><span class="hljs-comment"># 第二行代码会创建一个从 -2.0 到 2.1 结束（不包含 2.1），步长为 0.1 的数组。</span><br>w_range = np.arange(<span class="hljs-number">0.0</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">0.1</span>)<br>b_range = np.arange(-<span class="hljs-number">2.0</span>, <span class="hljs-number">2.1</span>, <span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 创建一个用于存储 MSE 值的二维网格</span><br><span class="hljs-comment"># 分别创建两个矩阵 w_values 和 b_values， 用于绘制 3D 图时的 x, y 坐标</span><br><span class="hljs-comment"># w_values：这个矩阵的每一 行 都是 w_range 数组的重复</span><br><span class="hljs-comment"># b_values：这个矩阵的每一 列 都是 b_range 数组的重复。</span><br><span class="hljs-comment"># 故此将两个矩阵重叠，相同位置的两个元素(w,b)就代表了所有可能的点</span><br>w_values, b_values = np.meshgrid(w_range, b_range)<br><br><span class="hljs-comment"># 创建全零矩阵保存所有的损失值</span><br><span class="hljs-comment"># np.zeros_like()：这个函数会创建一个与 w_values 形状完全相同（即行列数一样）的全零矩阵。</span><br>mse_values = np.zeros_like(w_values)<br><br><span class="hljs-comment"># 遍历 w 和 b 的所有组合，计算并存储 MSE 值</span><br><span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(w_range):<br>    <span class="hljs-keyword">for</span> j, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(b_range):<br>        mse_values[j, i] = MSE(x_data, y_data, w, b)<br><br><span class="hljs-comment"># 绘制 3D 图像</span><br><span class="hljs-comment"># figure函数 创建一个新的图形窗口</span><br><span class="hljs-comment"># figsize 参数设置了窗口的尺寸（长和宽），当然figsize不填也行</span><br>fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br><br><span class="hljs-comment"># add_subplot()函数 在图形窗口中添加一个子图（subplot）</span><br><span class="hljs-comment"># 111 表示将图形窗口划分为 1x1 的网格，并在第 1 个子图上绘图</span><br><span class="hljs-comment"># 下列方式是简便的工厂模式。告诉 add_subplot 函数你想要一个 3D 坐标系，它就会在后台为你创建一个 Axes3D 对象。</span><br><span class="hljs-comment"># projection=&#x27;3d&#x27; 告诉 Matplotlib 要创建一个 3D 坐标系，而不是默认的 2D 坐标系</span><br><span class="hljs-comment"># 这种方式代码更简洁，但可能会让 IDE 产生误判，导致最开始的import显示未引用</span><br>ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br><br><span class="hljs-comment"># 绘制曲面图</span><br><span class="hljs-comment"># plot_surface() 绘制曲面图函数</span><br><span class="hljs-comment"># w_values, b_values, mse_values：这三个参数是 3D 表面图的三个坐标轴：x 轴、y 轴和 z 轴</span><br><span class="hljs-comment"># cmap=cm.viridis：cmap 是 &quot;colormap&quot;（颜色映射）的缩写，它定义了曲面图的颜色方案。viridis 是一种常用的、颜色渐变平滑的方案</span><br><span class="hljs-comment"># alpha=0.9：alpha 设置了曲面的透明度，0.0 是完全透明，1.0 是完全不透明。</span><br>surf = ax.plot_surface(w_values, b_values, mse_values, cmap=cm.viridis, alpha=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-comment"># 设置图像标题和标签</span><br>ax.set_title(<span class="hljs-string">&quot;3D 损失函数（MSE）表面图&quot;</span>, fontsize=<span class="hljs-number">16</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;w (权重)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;b (截距)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.set_zlabel(<span class="hljs-string">&#x27;MSE (损失)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br><br><span class="hljs-comment"># 添加颜色条</span><br>fig.colorbar(surf, shrink=<span class="hljs-number">0.5</span>, aspect=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># 找到损失函数的最小值点（理论上，w=2.0, b=0.0 时损失为0）</span><br><span class="hljs-comment"># np.argmin()：这个函数返回一个数组中最小值元素的索引</span><br><span class="hljs-comment"># axis=None：忽略数组的维度，而是在整个二维矩阵中寻找唯一的一个最小值。它会返回这个最小值在一维展平（flattened）后的数组中的索引</span><br><span class="hljs-comment"># axis=0：表示沿着列（columns）进行操作，有几列就会返回几个元素的数组</span><br><span class="hljs-comment"># axis=1：表示沿着行（rows）进行操作，有几行就返回几个元素的数组</span><br><span class="hljs-comment"># 例如取None时 mse_values 矩阵是 [[10, 20], [3, 40]]，那么它的最小值是 3</span><br><span class="hljs-comment"># 在一维展平后，数组变为 [10, 20, 3, 40]，3 的索引是 2。所以 np.argmin() 会返回 2</span><br><span class="hljs-comment"># np.unravel_index()：反向将一维数组的索引转换为二维，但是要填入参数 shape：即我们传入的是原始矩阵的形状</span><br><span class="hljs-comment"># 最后返回的是二维坐标 (w, b)</span><br>min_mse_index = np.unravel_index(np.argmin(mse_values, axis=<span class="hljs-literal">None</span>), mse_values.shape)<br><span class="hljs-comment"># 在两个矩阵中获取最小MES对应的具体值</span><br>min_b = b_values[min_mse_index]<br>min_w = w_values[min_mse_index]<br><br><span class="hljs-comment"># 在图上标记最小值点</span><br><span class="hljs-comment"># 参数依次对应：颜色、形状、大小、标签说明</span><br>ax.scatter(min_w, min_b, mse_values.<span class="hljs-built_in">min</span>(), color=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, s=<span class="hljs-number">100</span>,<br>           label=<span class="hljs-string">f&#x27;最小损失点\n(w=<span class="hljs-subst">&#123;min_w:<span class="hljs-number">.2</span>f&#125;</span>, b=<span class="hljs-subst">&#123;min_b:<span class="hljs-number">.2</span>f&#125;</span>)&#x27;</span>)<br><br><span class="hljs-comment"># 显示散点图的图例</span><br>ax.legend()<br><br><span class="hljs-comment"># 调整视角以获得更好的可视化效果</span><br><span class="hljs-comment"># elev 是仰角，azim 是方位角</span><br>ax.view_init(elev=<span class="hljs-number">20</span>, azim=-<span class="hljs-number">120</span>)<br><br><span class="hljs-comment"># 显示最终绘制好的图形窗口</span><br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250917225211998.png" srcset="/img/loading.gif" lazyload alt="image-20250917225211998"></p>
</li>
</ul>
<h2 id="分治法求取多维权重（被舍弃的方法）"><a href="#分治法求取多维权重（被舍弃的方法）" class="headerlink" title="分治法求取多维权重（被舍弃的方法）"></a>分治法求取多维权重（被舍弃的方法）</h2><ul>
<li>当涉及的权重有多个时，以二维为例，假设每个权重取100个值，则一共有100^2种组合（维度的诅咒）</li>
<li>首先取一个稀疏点阵，比如 4 x 4 的稀疏点阵，求取这16个点的MSE值</li>
<li>再选取MSE最小的那个点，在它的周围再次取一个 4 x 4 的稀疏点阵，重复计算</li>
<li>以此试图找到使得MSE最小的权重组合</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<p>弊端：为什么被舍弃</p>
<ul>
<li>机器学习中的优化问题（如线性回归、神经网络训练）通常是<strong>连续可导</strong>的，不适合用分治法分成若干个小问题</li>
</ul>
</blockquote>
<h2 id="梯度下降算法（推荐）"><a href="#梯度下降算法（推荐）" class="headerlink" title="梯度下降算法（推荐）"></a>梯度下降算法（推荐）</h2><ul>
<li>先随机取一个权重组合</li>
<li>计算取该组合时，损失函数在此<strong>对权重</strong>的导数（也叫<strong>梯度</strong>）；<ul>
<li>导数为正，代表往右递增，往左递减</li>
<li>导数为负，代表往右递减，往左递增</li>
</ul>
</li>
<li>以导数为基准，对权重进行迭代，而不是像穷举法那样以相同的步长进行迭代</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918165530078.png" srcset="/img/loading.gif" lazyload alt="一维权重求导过程"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918165604233.png" srcset="/img/loading.gif" lazyload alt="权重迭代公式"></p>
<ul>
<li>权重迭代公式中的 <strong>α</strong> 的名称叫做<strong>学习率</strong>，是一个很重要的参数，相当于穷举法中的步幅<ul>
<li>它不是通过训练学习出来的，而是需要我们在训练前手动设置。选择一个合适的学习率是训练一个高性能模型的关键一步。</li>
<li>步子迈得太大，可能会直接<strong>跳过</strong>损失函数的最小值点，甚至在“山谷”的两侧来回震荡，导致损失值越来越大，永远无法收敛。这就像你下山时，每一步都跨得太大，结果不是跑到山脚下，而是跳到了对面的悬崖上。</li>
<li>步子迈得太小，虽然能够保证每一步都朝着正确的方向前进，但<strong>收敛速度会非常慢</strong>，需要进行大量的迭代才能到达最小值。这会极大地增加训练所需的时间和计算资源。这就像你在下山时，每一步都只挪动一小段距离，要花很长时间才能到达山脚。</li>
</ul>
</li>
</ul>
<h3 id="梯度下降算法的问题"><a href="#梯度下降算法的问题" class="headerlink" title="梯度下降算法的问题"></a>梯度下降算法的问题</h3><ul>
<li>同样是局部最优解，但是比分治法处理的更平滑，也更容易克服，比如我采用多轮次，每轮选取不同的初始点。</li>
<li>鞍点问题：<ul>
<li>一维权重时，也就是驻点，即导数为零的点，会导致迭代停止</li>
<li>多维权重时，可能会形成像马鞍（薯片）一样的曲面，从某个方向看，这个点是<strong>局部最高点</strong>（比如马鞍的前后方向），从另一个方向看，这个点又是<strong>局部最低点</strong>（比如马鞍的左右方向）。</li>
</ul>
</li>
</ul>
<h3 id="简化模型使用梯度下降算法预测权重（无截距）"><a href="#简化模型使用梯度下降算法预测权重（无截距）" class="headerlink" title="简化模型使用梯度下降算法预测权重（无截距）"></a>简化模型使用梯度下降算法预测权重（无截距）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>] <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>] <span class="hljs-comment"># 输出</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x*w<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># MSE损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MSE</span>(<span class="hljs-params">xd, yd, w</span>):<br>    loss_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        loss_sum += loss(x_val, y_val, w)<br>    <span class="hljs-keyword">return</span> loss_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 定义梯度函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">xd, yd, w</span>):<br>    grad_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(xd, yd):<br>        grad_sum += <span class="hljs-number">2</span> * x_val * (x_val * w - y_val)<br>    <span class="hljs-keyword">return</span> grad_sum / <span class="hljs-built_in">len</span>(xd)<br><br><span class="hljs-comment"># 初始随机权重</span><br>w = <span class="hljs-number">1.0</span><br><span class="hljs-comment"># 定义学习率</span><br>a = <span class="hljs-number">0.01</span><br><span class="hljs-comment"># 轮次值列表</span><br>epoch_list = []<br><span class="hljs-comment"># MSE误差列表</span><br>mse_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>,  forward(<span class="hljs-number">4</span>, w))<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    mes_val = MSE(x_data, y_data, w)<br>    mse_list.append(mes_val)<br>    grad_val = gradient(x_data, y_data, w)<br>    w -= a * grad_val<br>    epoch_list.append(epoch)<br>    <span class="hljs-comment"># 详细值</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, w: <span class="hljs-subst">&#123;w&#125;</span>, mes: <span class="hljs-subst">&#123;mes_val&#125;</span>&quot;</span>)<br>    <span class="hljs-comment"># 保留两位小数的值</span><br>    <span class="hljs-comment"># print(f&quot;epoch: &#123;epoch&#125;, w: &#123;round(w, 2)&#125;, mes: &#123;round(mes_val, 2)&#125;&quot;)</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br>plt.plot(epoch_list, mse_list)<br>plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;MES&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918204114132.png" srcset="/img/loading.gif" lazyload alt="image-20250918204114132"></p>
<h3 id="随机梯度下降算法-SGD"><a href="#随机梯度下降算法-SGD" class="headerlink" title="随机梯度下降算法(SGD)"></a>随机梯度下降算法(SGD)</h3><ul>
<li><p>上述的梯度算法，对于每次权重迭代的大小是由<strong>所有数据的损失汇总取平均值得到的误差</strong>来计算的，这样的方式在遇到鞍点时可能会停滞不前</p>
</li>
<li><p>而随机梯度算法，权重的迭代大小是由<strong>随机取一个样本（也可以按照顺序随机，也叫顺序随机梯度算法），用其损失对权重求导</strong>，这样的方式可能可以跨过鞍点，使权重继续前进</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918213038164.png" srcset="/img/loading.gif" lazyload alt="image-20250918213038164"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入计算和绘图用的包</span><br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>]  <span class="hljs-comment"># 输入</span><br>y_data = [<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>]  <span class="hljs-comment"># 输出</span><br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 单个样本的损失loss</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> (y_pred - y) * (y_pred - y)<br><br><span class="hljs-comment"># 定义梯度函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">x, y, w</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * x * (x * w - y)<br><br><span class="hljs-comment"># 初始化权重和学习率</span><br>w = <span class="hljs-number">1.0</span><br>a = <span class="hljs-number">0.01</span><br><br><span class="hljs-comment"># 用于记录每个轮次的平均损失</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 训练循环</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-comment"># 在每个轮次开始时，初始化总损失</span><br>    epoch_loss_sum = <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># 随机选择一个样本进行训练</span><br>    index = random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<span class="hljs-comment"># 这里是真随机</span><br>    x_val = x_data[index]<br>    y_val = y_data[index]<br><br>    <span class="hljs-comment"># 计算梯度并更新权重</span><br>    grad = gradient(x_val, y_val, w)<br>    w -= a * grad<br><br>    <span class="hljs-comment"># 计算当前样本的损失并累加到总损失中</span><br>    current_loss = loss(x_val, y_val, w)<br><br>    <span class="hljs-comment"># 因为是SGD，一个轮次只训练一个样本，所以直接记录当前损失即可</span><br>    epoch_loss = current_loss / <span class="hljs-number">1</span> <span class="hljs-comment"># loss的计算只是为了输出体现为0而已</span><br><br>    <span class="hljs-comment"># 打印当前轮次的信息</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, w = <span class="hljs-subst">&#123;w:<span class="hljs-number">.2</span>f&#125;</span>, loss = <span class="hljs-subst">&#123;epoch_loss:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-comment"># 记录每个轮次的平均损失和轮次数，用于绘图</span><br>    epochs_list.append(epoch)<br>    costs_list.append(epoch_loss)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<ul>
<li><p>输出结果（由于每次都取单独一个样本进行计算梯度，所以波动会很大，每次图像基本不一样）</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250918221902242.png" srcset="/img/loading.gif" lazyload alt="image-20250918221902242"></p>
</li>
</ul>
<h3 id="小批量梯度下降算法-Mini-BGD"><a href="#小批量梯度下降算法-Mini-BGD" class="headerlink" title="小批量梯度下降算法(Mini-BGD)"></a>小批量梯度下降算法(Mini-BGD)</h3><ul>
<li>现在普遍也称<strong>BGD</strong></li>
<li>比如训练集有8对数据，每个小批次为2，则会分为4个批次batch</li>
<li>梯度计算是取当前批次内的样本所有梯度取平均，再用这个<strong>平均梯度</strong>更新权重</li>
<li>计算并记录当前 <strong>Epoch 的最终性能指标（Cost 或 Loss）</strong>时，都遵循，使用<strong>更新后的权重 w</strong>，对<strong>整个数据集</strong>取 <strong>MSE（或平均损失）</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 假设数据集有8组</span><br>x_data = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>])<br>y_data = np.array([<span class="hljs-number">2.3</span>, <span class="hljs-number">4.1</span>, <span class="hljs-number">5.8</span>, <span class="hljs-number">8.4</span>, <span class="hljs-number">10.3</span>, <span class="hljs-number">11.7</span>, <span class="hljs-number">14.5</span>, <span class="hljs-number">15.9</span>])<br><br><span class="hljs-comment"># 用于计算 y 的预测值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x, w</span>):<br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 使用 NumPy 计算整个样本的平均损失</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg_loss</span>(<span class="hljs-params">x, y, w</span>):<br>    y_pred = forward(x, w)<br>    <span class="hljs-keyword">return</span> np.mean((y_pred - y) ** <span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 使用 NumPy 计算整个Batch的平均梯度</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg_gradient</span>(<span class="hljs-params">x, y, w</span>):<br>    <span class="hljs-keyword">return</span> np.mean(<span class="hljs-number">2</span> * x * (x * w - y))<br><br><span class="hljs-comment"># 初始化</span><br>w = <span class="hljs-number">1.0</span> <span class="hljs-comment"># 权重</span><br>a = <span class="hljs-number">0.0001</span> <span class="hljs-comment"># 学习率</span><br>batch_size = <span class="hljs-number">2</span> <span class="hljs-comment"># 批次Batch大小</span><br>num_samples = <span class="hljs-built_in">len</span>(x_data) <span class="hljs-comment"># 数据总组数</span><br>num_batches = num_samples // batch_size  <span class="hljs-comment"># 批次Batch数量</span><br><br><span class="hljs-comment"># 轮次列表 与 轮次的平均损失列表</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 训练循环</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>):<br>    <span class="hljs-comment"># 第一步：打乱数据</span><br>    indices = np.random.permutation(num_samples) <span class="hljs-comment"># 生成一个包含从 0 到 num_samples-1 的随机排列数组</span><br>    <span class="hljs-comment"># 每个轮次的 数据顺序都不同</span><br>    shuffled_x = x_data[indices]<br>    shuffled_y = y_data[indices]<br><br>    <span class="hljs-comment"># 第二步：创建批次Batch并循环</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):<br>        <span class="hljs-comment"># 得到当前批次Batch的 起始 和 终止 索引</span><br>        start_index = i * batch_size<br>        end_index = start_index + batch_size<br>        <span class="hljs-comment"># 获取当前批次Batch的数据</span><br>        batch_x = shuffled_x[start_index:end_index]<br>        batch_y = shuffled_y[start_index:end_index]<br><br>        <span class="hljs-comment"># 计算当前批次Batch的平均梯度</span><br>        avg_batch_grad = avg_gradient(batch_x, batch_y, w)<br><br>        <span class="hljs-comment"># 更新权重</span><br>        w -= a * avg_batch_grad<br><br>    <span class="hljs-comment"># 计算并记录当前轮次（epoch）的平均损失</span><br>    avg_epoch_loss = avg_loss(x_data, y_data, w)<br><br>    <span class="hljs-comment"># 记录 当前轮次 和 当前轮次 的平均损失至数组方便绘图</span><br>    epochs_list.append(epoch)<br>    costs_list.append(avg_epoch_loss)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, w = <span class="hljs-subst">&#123;w&#125;</span>, avg_loss = <span class="hljs-subst">&#123;avg_epoch_loss&#125;</span>&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>, w))<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919174947709.png" srcset="/img/loading.gif" lazyload alt="image-20250919174947709"></p>
<ul>
<li>上述代码在求取平均值时采用了np.mean()的方法，使代码更简洁。</li>
</ul>
<h1 id="复杂神经网络初体验"><a href="#复杂神经网络初体验" class="headerlink" title="复杂神经网络初体验"></a>复杂神经网络初体验</h1><p>上述所实现的模型都只是很简单的少量权重，少量层数的线性模型。</p>
<p>那考虑如下图的大量权重，大量层数的<strong>多隐层前馈全连接神经网络</strong></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919214751381.png" srcset="/img/loading.gif" lazyload alt="image-20250919214751381"></p>
<ul>
<li>x11 先变成 y11，得通过H1，H2，H3，H4，这四层权重</li>
<li>输入X，是一个 5 * 1 的矩阵，第一层H1，是一个 6 * 1 的矩阵</li>
<li>而矩阵X变换成矩阵H1，需要权重矩阵 W1 与 X 相乘，即 W * X &#x3D; H</li>
<li>也就是说 W1 是一个 6 * 5 的矩阵</li>
<li>即 [6 * 5] * [5, 1] &#x3D; [6 * 1]</li>
<li>同理 H1 变换至 H2，需要一个 [7 * 6] 的W2矩阵</li>
</ul>
<h1 id="反向梯度传播算法"><a href="#反向梯度传播算法" class="headerlink" title="反向梯度传播算法"></a>反向梯度传播算法</h1><p>上述的全连接神经网络，有好几百个权重，如果每个都采用先写解析式是很困难的，而且每层之间都是复合函数，这是个相当复杂的工作量。</p>
<p><strong>故此考虑，把这样一个复杂的神经网络，看作一个图，通过图来传播梯度，根据链式法则求取梯度，这就叫反向梯度传播算法。</strong></p>
<h2 id="两层简单图"><a href="#两层简单图" class="headerlink" title="两层简单图"></a>两层简单图</h2><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919221832160.png" srcset="/img/loading.gif" lazyload alt="image-20250919221832160"></p>
<ul>
<li><p>每一层的结构都是一样的</p>
</li>
<li><p>权重矩阵 W 与输入矩阵相乘，得到一个新的矩阵</p>
</li>
<li><p>新的矩阵与偏置值矩阵 b 相加，得到输出结果</p>
</li>
<li><p><strong>当前层的输出作为下一层的输入</strong></p>
</li>
<li><p>最终得到预测解析式，类似于迭代套娃</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919222136045.png" srcset="/img/loading.gif" lazyload alt="image-20250919222136045"></p>
</li>
<li><p>但是这样建立模型会有一个<strong>问题</strong>，因为不管你套娃多少层，都可以通过计算展开成单层模型的样子（又回到最初的起点），不管多少层都变成单一线性的，如下图</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919222551620.png" srcset="/img/loading.gif" lazyload alt="image-20250919222551620"></p>
</li>
</ul>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><ul>
<li><p>解决方法也很简单，即在每一层的输入之前，对上一层的输出加上一个非线性的变化函数，整个函数也叫做<strong>激活函数</strong>，这样就不能对迭代式子进行展开了</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250919222858405.png" srcset="/img/loading.gif" lazyload alt="image-20250919222858405"></p>
</li>
</ul>
<h2 id="手算最小简单图"><a href="#手算最小简单图" class="headerlink" title="手算最小简单图"></a>手算最小简单图</h2><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/3c740378c7b44b705869e7192572877.jpg" srcset="/img/loading.gif" lazyload alt="3c740378c7b44b705869e7192572877"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/bd0537a18fae60f6476a65d7d9e471c.jpg" srcset="/img/loading.gif" lazyload alt="bd0537a18fae60f6476a65d7d9e471c"></p>
<h2 id="Tensor的作用"><a href="#Tensor的作用" class="headerlink" title="Tensor的作用"></a>Tensor的作用</h2><p>Tensor 是一个多维数组，可以用来存储标量（0 维）、向量（1 维）、矩阵（2 维）乃至更高维度的数据。</p>
<ul>
<li><strong>存储所有数据类型</strong>：在 PyTorch 中，无论是模型的输入数据（如图像的像素值、文本的词向量），模型的参数（权重 W 和偏置 b），还是中间计算结果，<strong>一切皆为 Tensor</strong>。</li>
</ul>
<p>Tensor是 PyTorch 框架中<strong>动态计算图</strong>的基石，负责存储数据和梯度</p>
<ul>
<li>Tensor 是构建计算图的基本单元。在一个深度学习模型中，所有运算（如加、乘、矩阵乘法等）都是 Tensor 之间的操作<ul>
<li><strong>动态 (Dynamic)<strong>：这是 PyTorch 的关键特性。计算图是</strong>在运行时</strong>（即每次前向传播时）动态构建的。这意味着图的结构可以根据输入数据的不同或程序逻辑（如条件判断）而改变。</li>
</ul>
</li>
<li>Tensor 内部存储了两个至关重要的数值<ul>
<li>数据 (Data)：主体部分，即<strong>节点的值</strong>，用于存储实际的输入数据、模型参数（权重 W、b）以及前向传播过程中的所有中间结果。</li>
<li>梯度 (Grad)：Tensor 的 <code>.grad</code> 属性，存储了<strong>梯度值 (gradient)<strong>，这个值代表了</strong>损失函数 (loss)</strong> 对该 Tensor 的<strong>导数</strong></li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250924205707890.png" srcset="/img/loading.gif" lazyload alt="image-20250924205707890"></p>
<h2 id="使用Tensor实现反向传播"><a href="#使用Tensor实现反向传播" class="headerlink" title="使用Tensor实现反向传播"></a>使用Tensor实现反向传播</h2><p>关键代码详解：</p>
<ul>
<li><code>loss_val.backward()</code> 执行后，具体哪些对象及其值会发生改变？<ul>
<li>唯一会发生<strong>实质性改变</strong>的对象是<strong>具有 <code>requires_grad=True</code> 属性的叶子 Tensor</strong>，在本例中就是您的权重 w</li>
<li>w.grad：值被累加</li>
<li>w：值保持不变（w的值默认表示data）</li>
<li>其他中间 Tensor：值保持不变（<code>forward()</code> 过程中产生的中间 Tensor（如 <code>y_pred</code>）的值保持不变，它们所包含的梯度历史信息会被用于反向传播，然后通常会被释放。）</li>
</ul>
</li>
<li>为什么要使用 <strong>with torch.no_grad():</strong> 包裹更新权重的代码？<ul>
<li>防止构建计算图：梯度下降的目标是<strong>修改</strong>权重 <code>w</code> 的值，而不是将这个修改过程作为一次可微分的计算。<code>torch.no_grad()</code> 告诉 PyTorch：“以下操作只是数据管理，不要追踪。”</li>
<li>避免循环依赖和错误：如果没有 <code>no_grad()</code>，PyTorch 会将 <code>w_new = w_old - a * w.grad</code> 这个操作记录到计算图中。</li>
</ul>
</li>
<li>为什么梯度要使用 <code>w.grad.zero_()</code> 置零，以及为什么它不用被 <code>torch.no_grad()</code> 包裹？<ul>
<li><code>backward()</code> 方法的机制是<strong>累加梯度</strong>，除非确实需要累加，否则要显示的使用.zero_()置零</li>
<li><code>w.grad</code> 存储的是<strong>上一次计算的结果</strong>，它是一个<strong>非活跃的 Tensor</strong>。我们并不需要对梯度本身求梯度。PyTorch 已经明确地将对 <code>.grad</code> 属性执行的原地操作（如 <code>zero_()</code>）视为<strong>纯粹的数据管理</strong>，并默认允许它绕过梯度追踪系统。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], dtype=torch.float32)<br>y_data = torch.tensor([<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>], dtype=torch.float32)<br>a = <span class="hljs-number">0.01</span><br><br><span class="hljs-comment"># 创建一个权重tensor，将值用[]框起来</span><br>w = torch.tensor([<span class="hljs-number">1.0</span>])<br><span class="hljs-comment"># 启动自动计算loss对w的梯度，默认关闭</span><br>w.requires_grad_(<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-comment"># 这里刚开始 w 是个tensor，x 是个数</span><br>    <span class="hljs-comment"># 当他俩相乘时，乘号*会自动重载，把x也化为一个tensor对象</span><br>    <span class="hljs-comment"># 最终做乘法返回一个tensor</span><br>    <span class="hljs-keyword">return</span> x * w<br><br><span class="hljs-comment"># 每调用一次loss函数，就动态的构建了计算图</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y</span>):<br>    y_pred = forward(x)<br>    <span class="hljs-keyword">return</span> (y_pred - y) ** <span class="hljs-number">2</span><br><br><span class="hljs-comment"># 轮次列表 与 轮次的平均损失列表</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>,  forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(x_data, y_data):<br>        loss_val = loss(x_val, y_val)<br>        loss_val.backward() <span class="hljs-comment"># 将梯度反向传播至w的tensor，并且与该样本相关的计算图就会被释放</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x = <span class="hljs-subst">&#123;x_val&#125;</span>, y = <span class="hljs-subst">&#123;y_val&#125;</span>, w = <span class="hljs-subst">&#123;w.item()&#125;</span>, grad = <span class="hljs-subst">&#123;w.grad.item():<span class="hljs-number">.4</span>f&#125;</span>, loss = <span class="hljs-subst">&#123;loss_val.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            w -= a * w.grad <span class="hljs-comment"># 使用no_grad()包裹，使tensor只更新data值，不生成计算图</span><br><br>        w.grad.zero_() <span class="hljs-comment"># 更新完成后，将梯度置零，防止累加</span><br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># 计算整个数据集的均方误差 (MSE)</span><br>        final_epoch_loss = torch.mean(loss(x_data, y_data)).item()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;------------------ Epoch <span class="hljs-subst">&#123;epoch:03d&#125;</span> End ------------------&quot;</span>)<br>    epochs_list.append(epoch)<br>    costs_list.append(final_epoch_loss)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250924221417266.png" srcset="/img/loading.gif" lazyload alt="image-20250924221417266"></p>
<h2 id="课后作业：二维权重计算题手算与代码实现"><a href="#课后作业：二维权重计算题手算与代码实现" class="headerlink" title="课后作业：二维权重计算题手算与代码实现"></a>课后作业：二维权重计算题手算与代码实现</h2><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/2d4c3d1409871e726d1c69164a19260.jpg" srcset="/img/loading.gif" lazyload alt="2d4c3d1409871e726d1c69164a19260"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 定义数据集</span><br>x_data = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], dtype=torch.float32)<br>y_data = torch.tensor([<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>], dtype=torch.float32)<br>a = <span class="hljs-number">0.01</span><br><br>w1 = torch.tensor([<span class="hljs-number">1.0</span>])<br>w2 = torch.tensor([<span class="hljs-number">1.0</span>])<br>b = torch.tensor([<span class="hljs-number">1.0</span>])<br>w1.requires_grad_(<span class="hljs-literal">True</span>)<br>w2.requires_grad_(<span class="hljs-literal">True</span>)<br>b.requires_grad_(<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> w1 * x ** <span class="hljs-number">2</span> + w2 * x + b<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">x, y</span>):<br>    y_pred = forward(x)<br>    <span class="hljs-keyword">return</span> (y_pred - y) ** <span class="hljs-number">2</span><br><br><span class="hljs-comment"># 轮次列表 与 轮次的平均损失列表</span><br>epochs_list = []<br>costs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练前的预测值 x =&#x27;</span>, <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;y =&#x27;</span>,  forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5000</span>):<br>    <span class="hljs-keyword">for</span> x_val, y_val <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(x_data, y_data):<br>        loss_val = loss(x_val, y_val)<br>        loss_val.backward()<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            w1 -= a * w1.grad<br>            w2 -= a * w2.grad<br>            b -= a * b.grad<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x = <span class="hljs-subst">&#123;x_val&#125;</span>, y = <span class="hljs-subst">&#123;y_val&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;w1 = <span class="hljs-subst">&#123;w1.item():<span class="hljs-number">.3</span>f&#125;</span>, w1&#x27;s new grad = <span class="hljs-subst">&#123;w1.grad.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;w2 = <span class="hljs-subst">&#123;w2.item():<span class="hljs-number">.3</span>f&#125;</span>, w2&#x27;s new grad = <span class="hljs-subst">&#123;w2.grad.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;b = <span class="hljs-subst">&#123;b.item():<span class="hljs-number">.3</span>f&#125;</span>, b&#x27;s new grad = <span class="hljs-subst">&#123;b.grad.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span><br>              <span class="hljs-string">f&quot;single_loss = <span class="hljs-subst">&#123;loss_val.item():<span class="hljs-number">.3</span>f&#125;</span>\n&quot;</span>)<br><br>        w1.grad.zero_()<br>        w2.grad.zero_()<br>        b.grad.zero_()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># 计算整个数据集的均方误差 (MSE)</span><br>        final_epoch_loss = torch.mean(loss(x_data, y_data)).item()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;------------ Epoch <span class="hljs-subst">&#123;epoch:03d&#125;</span> End ----------- Epoch&#x27;s MSE: <span class="hljs-subst">&#123;final_epoch_loss:<span class="hljs-number">.4</span>f&#125;</span>-----------&quot;</span>)<br>    epochs_list.append(epoch)<br>    costs_list.append(final_epoch_loss)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练后的预测值 x = 4, y =&#x27;</span>, forward(<span class="hljs-number">4</span>).item())<br><br><span class="hljs-comment"># 绘制损失曲线</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20250925222226179.png" srcset="/img/loading.gif" lazyload alt="image-20250925222226179"></p>
<h2 id="阶段小结"><a href="#阶段小结" class="headerlink" title="阶段小结"></a>阶段小结</h2><h3 id="SGD与Mini-BGD、BGD的区别"><a href="#SGD与Mini-BGD、BGD的区别" class="headerlink" title="SGD与Mini-BGD、BGD的区别"></a>SGD与Mini-BGD、BGD的区别</h3><ul>
<li><p>在 <strong>Mini-batch 梯度下降 (MBGD)</strong> 中，<strong>梯度计算</strong>和 <strong>权重更新</strong> 都是基于<strong>当前批次</strong>的<strong>平均值</strong>。</p>
</li>
<li><p>SGD 的标准定义是：在每一次权重更新时，只使用<strong>一个</strong>样本的梯度。就算他是每个都算，并且按顺序算，但是他更新权重的时机是在单一样本后的。</p>
<ul>
<li>一个样本更新一次w</li>
</ul>
</li>
<li><p>BGD（Batch Gradient Descent，批量梯度下降）的定义是：在进行一次权重更新时，必须使用<strong>整个数据集</strong>的平均梯度。</p>
<ul>
<li>N个样本更新一次w</li>
</ul>
</li>
<li><p>Mini-BGD，<strong>梯度计算</strong>和 <strong>权重更新</strong> 都是基于<strong>当前批次</strong>的<strong>平均值</strong></p>
</li>
</ul>
<h3 id="计算当前Epoch的时机"><a href="#计算当前Epoch的时机" class="headerlink" title="计算当前Epoch的时机"></a>计算当前Epoch的时机</h3><ul>
<li>不管采用哪种梯度下降算法（BGD、MBGD、SGD），计算并记录当前 <strong>Epoch 的最终性能指标（Cost 或 Loss）</strong>时，都遵循，使用<strong>更新后的权重 w</strong>，对<strong>整个数据集</strong>取 <strong>MSE（或平均损失）</strong></li>
</ul>
<h1 id="使用PyTorch实现线性回归"><a href="#使用PyTorch实现线性回归" class="headerlink" title="使用PyTorch实现线性回归"></a>使用PyTorch实现线性回归</h1><h2 id="名词解释：张量、标量、向量、矩阵"><a href="#名词解释：张量、标量、向量、矩阵" class="headerlink" title="名词解释：张量、标量、向量、矩阵"></a>名词解释：张量、标量、向量、矩阵</h2><ul>
<li><p>张量：即前面提到的Tensor，是 PyTorch 存储数据和执行计算的基本单位，张量是一个<strong>多维数组</strong>，它是标量、向量和矩阵的广义统称。</p>
</li>
<li><p>标量：标量是一个<strong>零维张量</strong> (0-D Tensor) ，它只包含一个数值，没有方向或大小之分。</p>
<ul>
<li>torch.tensor(5.0)</li>
</ul>
</li>
<li><p>向量：向量是一个<strong>一维张量</strong> (1-D Tensor)，它包含一列有序的数值，只有大小和方向。</p>
<ul>
<li>torch.tensor([1, 2, 3])</li>
</ul>
</li>
<li><p>矩阵：矩阵是一个<strong>二维张量</strong> (2-D Tensor) ，它由行和列组成，是两个向量的排列。</p>
<ul>
<li>torch.tensor([[1, 2], [3, 4]])</li>
</ul>
</li>
</ul>
<p>	</p>
<blockquote>
<p>[!TIP]</p>
<p>几个 [] 就是几维张量</p>
</blockquote>
<h2 id="前置关键代码解释"><a href="#前置关键代码解释" class="headerlink" title="前置关键代码解释"></a>前置关键代码解释</h2><p><strong>nn.Linear(in_features, out_features, bias&#x3D;true)</strong></p>
<p>一共三个主要参数需要填写，Linear源码如下图：</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011191525307.png" srcset="/img/loading.gif" lazyload alt="image-20251011191525307"></p>
<ul>
<li><strong>in_features</strong>：输入特征数，或者说输入的维度，比如说你的输入矩阵是一个<strong>3*2</strong>的<strong>矩阵</strong>，每一行代表单个样本，也就是说一个样本有两个维度，即列数，那么这里就应该设置<strong>in_features&#x3D;2</strong></li>
<li><strong>out_features</strong>：输出特征数，同上。如果输出是一个<strong>4*3</strong>的矩阵，每一行代表单个样本，也就是有3个维度，那么就应该设置out_features&#x3D;<strong>3</strong></li>
<li><strong>bias</strong>：默认为<strong>True</strong>，用于是否要在计算过程中加上偏置项b</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p>这里有一个很重要的思想转变，即输入输出的值，<strong>从”数字”变成“矩阵”</strong></p>
<p>看网课时我一开始一直没理解老师为什么要一直使用矩阵的知识，我默认认为一个线性模型y &#x3D; w*x + b中的x、y、w、b都只是一个数字。</p>
<p>但是现在要转变观念，X可以是一个m * n的矩阵，Y可以是一个 m * q 的矩阵</p>
<p>那么又引申出一个问题，W应该是怎样的矩阵？</p>
<p>当使用PyTorch进行线性回归时，将会遵循以下乘法规则：</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011193345138.png" srcset="/img/loading.gif" lazyload alt="image-20251011193345138"></p>
</blockquote>
<p>至于为什么要对W进行转置，请往下看：</p>
<ul>
<li><p>当你输入nn.Linear(3，2)时，也就意味着你的输入数据集X_Data是形状是 <strong>N * 3</strong>，输出数据集Y_Data的形状是 <strong>N * 2</strong>，总共有<strong>N</strong>个样本</p>
</li>
<li><p>如果采用上图的乘法顺序，W的性质应该刚好是 <strong>3 * 2</strong>，和你括号中输入的顺序是一致的，但是由于PyTorch内部规定了需要取转置后再乘（如下图），所以为了符合直觉，源代码中特地反转了。</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011194135976.png" srcset="/img/loading.gif" lazyload alt="image-20251011194135976"></p>
</li>
</ul>
<p><strong>optimizer &#x3D; torch.optim.SGD(my_model.parameters(), lr&#x3D;0.01)</strong></p>
<ul>
<li>my_model.parameters()  的作用是返回一个迭代器 (iterator) ，其中包含了 <strong>my_model</strong> 实例中所有需要学习和更新的参数 (Parameter) 。</li>
<li>反正是pytorch帮忙找，用迭代找的，后续需要再深入学习。</li>
</ul>
<p><strong>执行my_model(x_data)会自动执行forward返回预测值</strong></p>
<h2 id="具体代码"><a href="#具体代码" class="headerlink" title="具体代码"></a>具体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn <span class="hljs-comment">## nn 是 neural network 神经网络的缩写</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment">#定义数据集，设置为矩阵模式，3*1 的矩阵</span><br>x_data = torch.tensor([[<span class="hljs-number">1.0</span>], [<span class="hljs-number">2.0</span>], [<span class="hljs-number">3.0</span>]], dtype=torch.<span class="hljs-built_in">float</span>)<br>y_data = torch.tensor([[<span class="hljs-number">2.0</span>], [<span class="hljs-number">4.0</span>], [<span class="hljs-number">6.0</span>]], dtype=torch.<span class="hljs-built_in">float</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LinearRegression</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 调用父类的初始化方法</span><br>        <span class="hljs-built_in">super</span>(LinearRegression, self).__init__()<br>        <span class="hljs-comment"># nn.Linear 自动创建并初始化 w 和 b，反正会自动，具体可以后续详细了解</span><br>        self.linear = nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 前馈</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 默认执行 y = xW^T + b.</span><br>        <span class="hljs-keyword">return</span> self.linear(x)<br><br>    <span class="hljs-comment"># 反馈，PyTorch会自动帮你求导，除非你自己写的效率比PyTorch还高</span><br><br><span class="hljs-comment"># 实例化自己的模型</span><br>my_model = LinearRegression()<br><span class="hljs-comment"># 均方误差损失函数</span><br>criterion = nn.MSELoss()<br><span class="hljs-comment"># 使用随机梯度下降优化器（即更新权重的算法）</span><br>optimizer = torch.optim.SGD(my_model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># 打印训练前的参数</span><br>w_init = my_model.linear.weight.item()<br>b_init = my_model.linear.bias.item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练前参数: w=<span class="hljs-subst">&#123;w_init:<span class="hljs-number">.4</span>f&#125;</span>, b=<span class="hljs-subst">&#123;b_init:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 3. 训练参数</span><br>num_epochs = <span class="hljs-number">500</span><br>costs_list = []<br>epochs_list = []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-comment"># 每次迭代都需要执行的操作：</span><br><br>    <span class="hljs-comment"># (A) 梯度清零：使用优化器内置的方法，替代 w.grad.zero_()</span><br>    optimizer.zero_grad()<br><br>    <span class="hljs-comment"># (B) 前向传播</span><br>    y_pred = my_model(x_data)<br><br>    <span class="hljs-comment"># (C) 计算损失</span><br>    loss = criterion(y_pred, y_data)<br><br>    <span class="hljs-comment"># (D) 反向传播：计算梯度</span><br>    loss.backward()<br><br>    <span class="hljs-comment"># (E) 更新权重：使用优化器内置的方法，替代 with torch.no_grad(): W -= lr * W.grad</span><br>    optimizer.step()<br><br>    <span class="hljs-comment"># 记录损失（使用更新后的权重计算）</span><br>    <span class="hljs-comment"># 如果不在意小精度，也可以直接costs_list.append(loss.item())，也就是使用旧的W</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        costs_list.append(criterion(my_model(x_data), y_data).item())<br>    epochs_list.append(epoch)<br><br>    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>        current_w = my_model.linear.weight.item()<br>        current_b = my_model.linear.bias.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch [<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>], Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.6</span>f&#125;</span>, w: <span class="hljs-subst">&#123;current_w:<span class="hljs-number">.4</span>f&#125;</span>, b: <span class="hljs-subst">&#123;current_b:<span class="hljs-number">.4</span>f&#125;</span>&#x27;</span>)<br><br><span class="hljs-comment"># 5. 最终预测和结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">40</span>)<br>final_w = my_model.linear.weight.item()<br>final_b = my_model.linear.bias.item()<br><br><span class="hljs-comment"># 预测 x=4</span><br>x_test = torch.tensor([[<span class="hljs-number">4.0</span>]], dtype=torch.float32)<br>y_test_pred = my_model(x_test).item()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练后参数: w=<span class="hljs-subst">&#123;final_w:<span class="hljs-number">.4</span>f&#125;</span>, b=<span class="hljs-subst">&#123;final_b:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练后预测 x=4, y_pred=<span class="hljs-subst">&#123;y_test_pred:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 6. 绘图</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;MSE Loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011200926371.png" srcset="/img/loading.gif" lazyload alt="训练结果最好的一次"></p>
<h3 id="使用其他优化器"><a href="#使用其他优化器" class="headerlink" title="使用其他优化器"></a>使用其他优化器</h3><p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011202737953.png" srcset="/img/loading.gif" lazyload alt="image-20251011202737953"></p>
<h1 id="逻辑斯蒂回归（Logistic-Regression）"><a href="#逻辑斯蒂回归（Logistic-Regression）" class="headerlink" title="逻辑斯蒂回归（Logistic Regression）"></a>逻辑斯蒂回归（Logistic Regression）</h1><p>逻辑斯蒂回归本质上是一个<strong>二分类</strong>算法，并不是像线性回归那样输出一个具体的值，而是输出该样本属于某个分类的<strong>概率</strong></p>
<ul>
<li><p>例如手写的0~9的数据集，难道要拿到一张手写图，直接输出这是具体的几吗？并不是，而是输出它是0的概率P(0)，它是1的概率P(1)……，最终选择概率最大的</p>
</li>
<li><p>而逻辑斯蒂回归主要解决的是二分类，也就是只有两个分类。</p>
</li>
<li><p>由于概率总和肯定等于1，所以我们只需要计算其中一个分类的概率即可。</p>
</li>
<li><p>而逻辑斯蒂回归的核心在于将线性回归的结果通过一个<strong>非线性函数</strong>——<strong>Sigmoid 函数（或 Logit 函数）</strong>——映射到 (0,1) 的概率区间。<strong>所以要先进行线性回归</strong></p>
</li>
<li><p>其中最经典的函数是<strong>Logit 函数</strong>（如下图），它完美的把整个实数域限制在了[-1, 1]之间，当然还有很多其他的<strong>Sigmoid 函数</strong>，但是由于<strong>Logit 函数</strong>太过经典，<strong>所以一般说Sigmoid 函数时，往往代指的就是Logit 函数</strong></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011213706761.png" srcset="/img/loading.gif" lazyload alt="最经典的Logit函数"></p>
</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011213740427.png" srcset="/img/loading.gif" lazyload alt="其他的Sigmoid函数"></p>
<ul>
<li>与线性回归对比，回顾一下前面提到的<strong>激活函数</strong>，以及为什么需要激活函数</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011213958565.png" srcset="/img/loading.gif" lazyload alt="image-20251011213958565"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011214036393.png" srcset="/img/loading.gif" lazyload alt="image-20251011214036393"></p>
<h2 id="损失函数的改变：BCE"><a href="#损失函数的改变：BCE" class="headerlink" title="损失函数的改变：BCE"></a>损失函数的改变：BCE</h2><ul>
<li><p>原本线性回归的MSE是求两个具体值的差值，而逻辑斯蒂得到的是两个概率分布，而概率分布是不能直接相减的</p>
</li>
<li><p>而求概率的差值用的是交叉熵，举例如下：</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251014223102425.png" srcset="/img/loading.gif" lazyload alt="image-20251014223102425"></p>
</li>
<li><p>而逻辑斯蒂是二分类问题，只有两种分布，真实值的分布和预测值的分布，即y和ŷ，那么得到的形式如下：</p>
</li>
</ul>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251011214218535.png" srcset="/img/loading.gif" lazyload alt="image-20251011214218535"></p>
<ul>
<li>当 <strong>y&#x3D;1</strong> 时，即真实值属于<strong>class_1</strong>，(1-y) &#x3D; 0，则损失函数就变成了：<strong>loss &#x3D; -log ŷ</strong>，而损失是要越小越好，由于是前面有负号，则表示<strong>log ŷ</strong>越大越好，根据对数函数的图像，且定义域 ŷ 属于[0, 1]，所以当 <strong>ŷ 越接近 1</strong> 时，loss的值越小越接近0，也就代表 ŷ 越接近真实值 <strong>y &#x3D; 1</strong>；</li>
<li>当 y &#x3D; 0 时反之；</li>
</ul>
<h2 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn <span class="hljs-comment">## nn 是 neural network 神经网络的缩写</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment">#定义数据集，设置为矩阵模式，3*1 的矩阵</span><br><span class="hljs-comment"># X 表示学习时长</span><br>x_data = torch.tensor([[<span class="hljs-number">1.0</span>], [<span class="hljs-number">2.0</span>], [<span class="hljs-number">3.0</span>]], dtype=torch.<span class="hljs-built_in">float</span>)<br><span class="hljs-comment"># Y 表示是否及格</span><br>y_data = torch.tensor([[<span class="hljs-number">0.0</span>], [<span class="hljs-number">0.0</span>], [<span class="hljs-number">1.0</span>]], dtype=torch.<span class="hljs-built_in">float</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LogisticRegressionModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 调用父类的初始化方法</span><br>        <span class="hljs-built_in">super</span>(LogisticRegressionModel, self).__init__()<br>        <span class="hljs-comment"># nn.Linear 自动创建并初始化 w 和 b，反正会自动，具体可以后续详细了解</span><br>        self.linear = nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 前馈</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> F.sigmoid(self.linear(x))<br><br>    <span class="hljs-comment"># 反馈，PyTorch会自动帮你求导，除非你自己写的效率比PyTorch还高</span><br><br><span class="hljs-comment"># 实例化自己的模型</span><br>my_model = LogisticRegressionModel()<br><span class="hljs-comment"># 使用 BCELoss (二元交叉熵损失)</span><br>criterion = nn.BCELoss()<br><span class="hljs-comment"># 使用 SGD 优化器，优化模型的参数 (w 和 b)</span><br>optimizer = optim.SGD(my_model.parameters(), lr=<span class="hljs-number">0.05</span>)<br><br><span class="hljs-comment"># 打印训练前的参数</span><br>w_init = my_model.linear.weight.item()<br>b_init = my_model.linear.bias.item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练前参数: w=<span class="hljs-subst">&#123;w_init:<span class="hljs-number">.4</span>f&#125;</span>, b=<span class="hljs-subst">&#123;b_init:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 训练参数及记录</span><br>num_epochs = <span class="hljs-number">2000</span><br>costs_list = []<br>epochs_list = []<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--- 开始训练 ---&quot;</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br><br>    <span class="hljs-comment"># (A) 梯度清零</span><br>    optimizer.zero_grad()<br><br>    <span class="hljs-comment"># (B) 前向传播: 得到预测概率 (Y_pred)</span><br>    y_pred = my_model(x_data)<br><br>    <span class="hljs-comment"># (C) 计算损失</span><br>    loss = criterion(y_pred, y_data)<br><br>    <span class="hljs-comment"># (D) 反向传播: 计算梯度</span><br>    loss.backward()<br><br>    <span class="hljs-comment"># (E) 更新权重</span><br>    optimizer.step()<br><br>    <span class="hljs-comment"># 记录损失</span><br>    costs_list.append(loss.item())<br>    epochs_list.append(epoch)<br><br>    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:<br>        current_w = my_model.linear.weight.item()<br>        current_b = my_model.linear.bias.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch [<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>], Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.6</span>f&#125;</span>, w: <span class="hljs-subst">&#123;current_w:<span class="hljs-number">.4</span>f&#125;</span>, b: <span class="hljs-subst">&#123;current_b:<span class="hljs-number">.4</span>f&#125;</span>&#x27;</span>)<br><br><span class="hljs-comment"># 6. 最终预测和结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">30</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--- 训练结束，查看结果 ---&quot;</span>)<br><br><span class="hljs-comment"># 切换到评估模式，并禁用梯度计算</span><br>my_model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-comment"># 预测整个数据集的概率</span><br>    y_prob = my_model(x_data)<br>    <span class="hljs-comment"># 将概率转换为类别 (P &gt;= 0.5 视为及格 1)</span><br>    y_predicted_classes = (y_prob &gt;= <span class="hljs-number">0.5</span>).<span class="hljs-built_in">float</span>()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;学习时长 (X):\n<span class="hljs-subst">&#123;x_data.squeeze()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测概率 (P):\n<span class="hljs-subst">&#123;y_prob.squeeze()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测类别 (Y):\n<span class="hljs-subst">&#123;y_predicted_classes.squeeze()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;真实类别 (Y_data):\n<span class="hljs-subst">&#123;y_data.squeeze()&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 7. 绘图 (可选: 可视化损失曲线)</span><br>plt.plot(epochs_list, costs_list)<br>plt.ylabel(<span class="hljs-string">&#x27;BCE Loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.show()<br><br><span class="hljs-comment"># 在 0 到 10 的范围内均匀地生成 200 个数据点</span><br>x = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">200</span>)<br><span class="hljs-comment"># 把 x 用Tensor转换成 200 * 1 的矩阵</span><br>x_test = torch.Tensor(x).view(<span class="hljs-number">200</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># 用训练好的模型输出y_test</span><br>y_test = my_model(x_test)<br><span class="hljs-comment"># Tensor 转列表</span><br>y = y_test.data.numpy()<br>plt.plot(x, y)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">10</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>], c=<span class="hljs-string">&#x27;r&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Hours&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Probability of Pass&#x27;</span>)<br>plt.grid()<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251014233538882.png" srcset="/img/loading.gif" lazyload alt="image-20251014233538882"></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251014233447064.png" srcset="/img/loading.gif" lazyload alt="image-20251014233447064"></p>
<h1 id="多维特征的输入（示例：疾病风险预测）"><a href="#多维特征的输入（示例：疾病风险预测）" class="headerlink" title="多维特征的输入（示例：疾病风险预测）"></a>多维特征的输入（示例：疾病风险预测）</h1><ul>
<li><p>其实前面已经提到了，线性模型中的输入输出数据集可以不在是一个具体的数，而是可以是多维的矩阵</p>
</li>
<li><p>采用矩阵加广播机制，可以不再使用for循环来训练，减轻代码量</p>
</li>
<li><p>而此时可以回答<strong>复杂神经网络</strong>的问题，它中间有很多层，每层都是一次线性回归，但是层与层之间要加上激活函数</p>
</li>
<li><p>当然你也可以手动的增加层数，比如你的输入集是8维的，输出集是一维的，直接使用（8 * 1）的权重当然可以，但是这样泛化能力不强。所以可以在中间加上几层神经网络，比如8–10–6–4—2–1，不但可以降维，甚至可以升维。</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251015201229862.png" srcset="/img/loading.gif" lazyload alt="image-20251015201229862"></p>
</li>
</ul>
<h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><p>主要训练代码和逻辑斯蒂回归一致，主要变化在于数据集的准备和构造模型上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">data = np.loadtxt(<span class="hljs-string">&#x27;DataSet/diabetes.csv.gz&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, dtype=np.float32)<br>x_data = torch.from_numpy(data[:, :-<span class="hljs-number">1</span>]) <span class="hljs-comment"># 所有行，除了最后一列</span><br>y_data = torch.from_numpy(data[:, [-<span class="hljs-number">1</span>]]) <span class="hljs-comment"># 所有行，最后一列，加[]的原因是防止numpy自动把矩阵降级成向量</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MDIModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MDIModel, self).__init__()<br>        self.linear1 = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>)<br>        self.linear2 = nn.Linear(<span class="hljs-number">6</span>, <span class="hljs-number">4</span>)<br>        self.linear3 = nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">1</span>)<br>        self.sigmoid = nn.Sigmoid()<br>        self.relu = nn.ReLU()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 注意：连续使用 Sigmoid 可能会导致梯度消失，一般最后一层才用逻辑斯蒂</span><br>        <span class="hljs-comment"># 现代网络中内部层通常使用 ReLU</span><br>        x = self.relu(self.linear1(x))<br>        x = self.relu(self.linear2(x))<br>        x = self.sigmoid(self.linear3(x))<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251015205313987.png" srcset="/img/loading.gif" lazyload alt="image-20251015205313987"></p>
<h1 id="Dataset-and-DataLoader"><a href="#Dataset-and-DataLoader" class="headerlink" title="Dataset and DataLoader"></a>Dataset and DataLoader</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ul>
<li>总不能还在代码里面手写数据集吧，更何况数据集还可能包括图片，语言这种形式</li>
<li>Dataset是用来构造一个数据集的，在torch中是一个抽象类，需要自定义类然后继承它</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiabetesDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filepath</span>):<br>        xy = np.loadtxt(filepath, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, dtype=np.float32)<br>        self.<span class="hljs-built_in">len</span> = xy.shape[<span class="hljs-number">0</span>]<br>        self.x_data = torch.from_numpy(xy[:, :-<span class="hljs-number">1</span>])<br>        self.y_data = torch.from_numpy(xy[:, [-<span class="hljs-number">1</span>]])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> self.x_data[index], self.y_data[index]<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.<span class="hljs-built_in">len</span><br></code></pre></td></tr></table></figure>

<ul>
<li>init方法，如果数据集小可以像上述一样直接全部加载到内存里，如果过大就要采用特殊的方法，比如类似页表？</li>
<li>getitem方法，根据索引返回单个样本，<strong>dataset[index]</strong></li>
<li>len方法，返回数据集的样本个数</li>
</ul>
<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><ul>
<li><p>主要是用来实现将数据集分批加载</p>
</li>
<li><p>基本概念</p>
<ul>
<li><p>Epoch（轮次）：<strong>一个完整的训练周期。</strong> 当所有训练数据在神经网络中完整地进行了一次前向传播（计算预测值）和一次反向传播（更新权重）时，就完成了一个 Epoch。Epoch 标志着模型对整个数据集的学习次数。通常需要多个 Epoch（如 100 次、1000 次）才能使模型收敛。</p>
</li>
<li><p>Batch Size（批次大小）：<strong>每次模型（权重）更新时使用的样本数量。</strong> 由于完整数据集通常太大，无法一次性放入内存或进行高效计算，因此训练数据会被分成许多小批次（Batch）。决定了权重的更新频率和稳定性。</p>
<ul>
<li><strong>小 Batch Size (如 1 - 32)：</strong> 权重更新频繁，但梯度计算有较大随机性（SGD）。</li>
<li><strong>大 Batch Size (如 64 - 256)：</strong> 权重更新次数少，梯度计算更稳定（近似 BGD）。</li>
</ul>
</li>
<li><p>Iteration（迭代）：<strong>完成一个 Epoch 所需的权重更新次数。</strong> 每处理完一个 Batch 的数据，就会完成一次迭代，并更新一次模型的权重。<strong>在数量上等于batch的个数</strong></p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251015221336256.png" srcset="/img/loading.gif" lazyload alt="image-20251015221336256"></p>
</li>
</ul>
</li>
<li><p>基本参数</p>
<ul>
<li>dataset：数据集的实例</li>
<li>batch_size：batch的大小</li>
<li>shuffle：是否打乱数据集</li>
<li>num_workers：训练时的线程数量</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_loader = DataLoader(dataset=dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251015221814219.png" srcset="/img/loading.gif" lazyload alt="image-20251015221814219"></p>
<h2 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiabetesDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filepath</span>):<br>        xy = np.loadtxt(filepath, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, dtype=np.float32)<br>        self.<span class="hljs-built_in">len</span> = xy.shape[<span class="hljs-number">0</span>]<br>        self.x_data = torch.from_numpy(xy[:, :-<span class="hljs-number">1</span>])<br>        self.y_data = torch.from_numpy(xy[:, [-<span class="hljs-number">1</span>]])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> self.x_data[index], self.y_data[index]<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.<span class="hljs-built_in">len</span><br><br>dataset = DiabetesDataset(<span class="hljs-string">&#x27;DataSet/diabetes.csv.gz&#x27;</span>)<br>train_loader = DataLoader(dataset=dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MDIModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MDIModel, self).__init__()<br>        self.linear1 = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>)<br>        self.linear2 = nn.Linear(<span class="hljs-number">6</span>, <span class="hljs-number">4</span>)<br>        self.linear3 = nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">1</span>)<br>        self.sigmoid = nn.Sigmoid()<br>        self.relu = nn.ReLU()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.relu(self.linear1(x))<br>        x = self.relu(self.linear2(x))<br>        x = self.sigmoid(self.linear3(x))<br>        <span class="hljs-keyword">return</span> x<br><br>my_model = MDIModel()<br><span class="hljs-comment"># criterion = nn.BCELoss(size_average=True) # &lt;-- 旧写法</span><br>criterion = nn.BCELoss(reduction=<span class="hljs-string">&#x27;mean&#x27;</span>) <span class="hljs-comment"># &lt;-- 新写法</span><br>optimizer = optim.SGD(my_model.parameters(), lr=<span class="hljs-number">0.05</span>)<br><br><span class="hljs-comment"># 训练参数及记录</span><br>num_epochs = <span class="hljs-number">500</span><br>costs_list = []<br>epochs_list = []<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        total_batch_loss = <span class="hljs-number">0.0</span><br>        num_batches = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader, <span class="hljs-number">0</span>):<br>            x_data, y_data = data <span class="hljs-comment"># 自动把x和y从train_loader中分离出来，并且是二维Tensor</span><br>            optimizer.zero_grad()<br>            y_pred = my_model(x_data)<br>            loss = criterion(y_pred, y_data)<br><br>            total_batch_loss += loss.item()  <span class="hljs-comment"># 累加本 Batch 的损失</span><br>            num_batches += <span class="hljs-number">1</span><br><br>            loss.backward()<br>            optimizer.step()<br>        <span class="hljs-comment"># 在 Epoch 结束后，计算平均损失</span><br>        <span class="hljs-comment"># 这样虽然不是特别准确, 但是胜在简洁高效，适用于训练集，如果是测试集则需要更准确的loss统计</span><br>        average_epoch_loss = total_batch_loss / num_batches<br><br>        <span class="hljs-comment"># 记录 Epoch 损失</span><br>        costs_list.append(average_epoch_loss)<br>        epochs_list.append(epoch)<br><br>        <span class="hljs-comment"># 可以输出 Epoch 结果</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch [<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>], Avg Loss: <span class="hljs-subst">&#123;average_epoch_loss:<span class="hljs-number">.6</span>f&#125;</span>&#x27;</span>)<br><br>    plt.plot(epochs_list, costs_list)<br>    plt.ylabel(<span class="hljs-string">&#x27;BCE Loss&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251015224237023.png" srcset="/img/loading.gif" lazyload alt="image-20251015224237023"></p>
<h2 id="课后练习：Kaggle之Titanic存活预测"><a href="#课后练习：Kaggle之Titanic存活预测" class="headerlink" title="课后练习：Kaggle之Titanic存活预测"></a>课后练习：Kaggle之Titanic存活预测</h2><p>篇幅问题：见本博客另一篇博文：<a href="https://blog.gutaicheng.top/2025/10/19/Kaggle%E7%AB%9E%E8%B5%9B-Titanic/">Kaggle竞赛-Titanic - GuTaicheng’s Blog</a></p>
<h1 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h1><ul>
<li><p>多分类问题，从字面意思来看，即最终会将数据集划分为2个以上的分类</p>
</li>
<li><p>比如说手写数字识别，最终就有十个分类，0~9</p>
</li>
<li><p>如果在神经网络的最后一层，仍然采用sigmoid函数输出的话，将会产生以下情况</p>
<ul>
<li>P(class&#x3D;0) &#x3D; 0.8; P(class&#x3D;1) &#x3D; 0.7;  P(class&#x3D;2) &#x3D; 0.9;  ……</li>
<li>因为sigmoid函数处理的是分类问题，也就是只能回答是或者不是，十种分类，最终会输出十个是或者不是</li>
<li>那到底概率大于多少才输出是呢？按照上面的输出，80%的概率是0，90%的概率是2，这样就完全无法分辨最终结果。</li>
<li>所以为了解决这个问题，引入全新的<strong>Softmax层</strong></li>
</ul>
</li>
<li><p>Softmax层的主要工作就是将多个线性输出的值，映射成：</p>
<ul>
<li>全部大于0</li>
<li>且总和等于1</li>
</ul>
</li>
<li><p>即采用如下的函数（其中Zi表示第i个线性输出的值），通过e^x，确保值为正，分母取总和，确保概率总和等于1</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251016203155514.png" srcset="/img/loading.gif" lazyload alt="image-20251016203155514"></p>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>一般来说多分类问题有两种最终层+损失函数的组合</p>
<ul>
<li><p>linear + CrossEntropyLoss</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251016203917787.png" srcset="/img/loading.gif" lazyload alt="image-20251016203917787"></p>
</li>
<li><p>LogSoftmax + NLLLoss</p>
<p><img src="https://gitee.com/Gu-taicheng/image/raw/master/img/image-20251016204011312.png" srcset="/img/loading.gif" lazyload alt="image-20251016204011312"></p>
<p>从图示很容易看出来，中间取对数是否被包含在计算损失的过程中</p>
</li>
</ul>
<h2 id="示例代码（MNIST手写数字集）"><a href="#示例代码（MNIST手写数字集）" class="headerlink" title="示例代码（MNIST手写数字集）"></a>示例代码（MNIST手写数字集）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br>batch_size = <span class="hljs-number">64</span><br>transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))<br>])<br><br>train_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;DataSet/MNIST&#x27;</span>,<br>                               train=<span class="hljs-literal">True</span>,<br>                               transform=transform,<br>                               download=<span class="hljs-literal">True</span>)<br><br>train_loader = DataLoader(dataset=train_dataset,<br>                          shuffle=<span class="hljs-literal">True</span>,<br>                          batch_size=batch_size)<br><br>test_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;DataSet/MNIST&#x27;</span>,<br>                              train=<span class="hljs-literal">False</span>,<br>                              transform=transform,<br>                              download=<span class="hljs-literal">True</span>)<br><br>test_loader = DataLoader(dataset=test_dataset,<br>                         shuffle=<span class="hljs-literal">False</span>,<br>                         batch_size=batch_size)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.l1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">512</span>)<br>        self.l2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>)<br>        self.l3 = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>)<br>        self.l4 = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>)<br>        self.l5 = nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        self.relu = nn.ReLU()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">784</span>)<br>        x = self.relu(self.l1(x))<br>        x = self.relu(self.l2(x))<br>        x = self.relu(self.l3(x))<br>        x = self.relu(self.l4(x))<br>        <span class="hljs-keyword">return</span> self.l5(x)<br><br>my_model = Net()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(my_model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch</span>):<br>    running_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> batch_idx, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader, <span class="hljs-number">0</span>):<br>        inputs, target = data<br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># forward + backward + update</span><br>        outputs = my_model(inputs)<br>        loss = criterion(outputs, target)<br>        loss.backward()<br>        optimizer.step()<br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> batch_idx % <span class="hljs-number">300</span> == <span class="hljs-number">299</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="hljs-number">1</span>, batch_idx + <span class="hljs-number">1</span>, running_loss / <span class="hljs-number">300</span>))<br>        running_loss = <span class="hljs-number">0.0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>():<br>    correct = <span class="hljs-number">0</span><br>    total = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>            images, labels = data<br>            outputs = my_model(images)<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs, dim=<span class="hljs-number">1</span>)<br>            total += labels.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="hljs-number">100</span> * correct / total))<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-keyword">for</span> epoch_num <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        train(epoch_num)<br>        test()<br></code></pre></td></tr></table></figure>

<h2 id="部分代码解释"><a href="#部分代码解释" class="headerlink" title="部分代码解释"></a>部分代码解释</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))<br>])<br></code></pre></td></tr></table></figure>

<ul>
<li>transforms.Compose([…])：定义一个初始化初始数据的<strong>流水线操作</strong>，然后作为参数设置到后面的DataLoader中</li>
<li>transforms.ToTensor()：<ul>
<li>将输入图像（通常是 PIL 图像或 NumPy <code>uint8</code> 数组，像素值范围是 0-255 ）转换为 PyTorch 的 <code>FloatTensor</code></li>
<li>在转换过程中，它自动将像素值从整数范围 0-255 <strong>归一化</strong>到浮点数范围 [0.0, 1.0]</li>
<li>将图像的维度顺序从传统的 (H, W, C)（高、宽、通道数）转换为 PyTorch 所需的 (C, H, W)（通道数、高、宽）。对于 MNIST，形状变为 (1, 28, 28)<ul>
<li>通道：说简单点就是这个样本是由几个矩阵叠加起来的。一张正常的彩色图片，就有3个通道，即三原色。而MNIST中是黑白图片，所以只有一个通道</li>
<li>高，宽28，指的是MNIST中当个数字样本的高宽有几个像素点</li>
</ul>
</li>
</ul>
</li>
<li>transforms.Normalize((0.1307,), (0.3081,))：对张量进行<strong>标准化</strong>。它进一步将像素值调整到具有<strong>零均值</strong>和<strong>单位方差</strong>的分布，以提高模型训练效率和收敛速度。<ul>
<li><code>(0.1307,)</code>：MNIST 训练集所有像素的<strong>平均值mean</strong></li>
<li><code>(0.3081,)</code>：MNIST 训练集所有像素的<strong>标准差std</strong></li>
<li>调整公式为：<strong>（原像素值 - mean）&#x2F; std</strong></li>
</ul>
</li>
<li>所以最终的流水线是<ul>
<li>拿到图片</li>
<li>变成[28 * 28 * 1]的三维张量，且值域为0-255</li>
<li>将值域归一化到浮点数范围 [0.0, 1.0]</li>
<li>维度调整为[1 * 28 * 28]</li>
<li>标准化像素值</li>
<li>之后每张图片都会变成同上规格的张量</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">784</span>)<br><br><span class="hljs-comment"># x实际上是 input</span><br>    <span class="hljs-keyword">for</span> batch_idx, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader, <span class="hljs-number">0</span>):<br>        inputs, target = data<br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># forward + backward + update</span><br>        outputs = my_model(inputs)<br></code></pre></td></tr></table></figure>

<ul>
<li><code>x</code> 此时是图像张量，其形状为 (Batch Size, 1, 28, 28)。但模型的第一层是全连接层 (<code>nn.Linear</code>)，它需要一个<strong>扁平化</strong>的输入向量。即矩阵</li>
<li><strong><code>view(...)</code> 的作用：</strong> 它是 PyTorch 中用于<strong>重塑 (Reshape)</strong> 张量的方法。</li>
<li><strong><code>-1</code> 的作用：</strong> -1是一个占位符。它告诉 PyTorch <strong>“自动计算这个维度的大小”</strong>，也就是batch的大小</li>
<li><strong><code>784</code> 的作用：</strong>28 * 28 &#x3D; 784。这表示将每个图像的1 * 28 * 28个像素<strong>展平</strong>为一个长度为 784 的向量。<strong>一行一行展开</strong></li>
<li><strong>效果：</strong> 将输入张量的形状从 (Batch Size, 1, 28, 28) 重塑为 (Batch Size, 784)，使之符合第一个线性层 <code>self.l1 = nn.Linear(784, 512)</code> 的输入要求。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = optim.SGD(my_model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure>

<p><strong><code>momentum=0.5</code> (动量)：</strong></p>
<ul>
<li><strong>含义：</strong> SGD 的一个优化项。它在更新时会考虑<strong>历史梯度</strong>（即之前的更新方向），而不是只依赖于当前的 Batch 梯度。</li>
<li><strong>作用：</strong> 帮助优化器克服梯度中的小波动和局部极小值，加速 SGD 在“平坦”区域的收敛，并减少训练时的震荡。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">_, predicted = torch.<span class="hljs-built_in">max</span>(outputs, dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<p><strong><code>torch.max(..., dim=1)</code>：</strong></p>
<ul>
<li><strong>作用：</strong> 找出 Tensor 在指定维度上的最大值和对应的索引。</li>
<li><strong><code>dim=1</code>：</strong> 对每一行（即对每个样本的 10 个类别得分）进行操作。等于0则表示按列操作</li>
<li><strong>返回结果：</strong> 它返回一个包含两个 Tensor 的元组：<code>(max_values, max_indices)</code>。</li>
<li><strong><code>_</code>：</strong> 占位符，忽略最大值本身。</li>
<li><strong><code>predicted</code>：</strong> 存储最大值所在的<strong>索引</strong>。这个索引（0 到 9）就是模型预测的类别。</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Pytorch-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#Pytorch - 深度学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Pytorch深度学习实践</div>
      <div>https://blog.gutaicheng.top/2025/09/16/Pytorch深度学习实践/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>GuTaicheng</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年9月16日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/19/Kaggle%E7%AB%9E%E8%B5%9B-Titanic/" title="Kaggle竞赛-Titanic">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Kaggle竞赛-Titanic</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/09/16/Conda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/" title="Conda使用教程">
                        <span class="hidden-mobile">Conda使用教程</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"Mm7zXkSdUNoJGtCjXx5EokSc-gzGzoHsz","appKey":"QeD6ibBU3GKctfSty5fFG9Xz","path":"window.location.pathname","placeholder":"什么都不用填就可以评论啦(当然留个qq或者邮箱更好哦)","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
